https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white


Proyecto de aprendizaje progresivo en Big Data y procesamiento distribuido, desde fundamentos hasta tÃ©cnicas avanzadas de Data Engineering con PySpark y Delta Lake.


ğŸ“Š DescripciÃ³n del Proyecto
Este proyecto representa un recorrido completo desde cero hasta experto en PySpark y Delta Lake, desarrollado en Databricks. Cubre 13 niveles progresivos de complejidad, desde operaciones bÃ¡sicas hasta arquitecturas de datos avanzadas con streaming, CDC (Change Data Capture) y optimizaciÃ³n de performance.
Objetivo: Demostrar dominio tÃ©cnico en ingenierÃ­a de datos mediante la implementaciÃ³n de casos de uso reales y mejores prÃ¡cticas de la industria.

ğŸ¯ Competencias TÃ©cnicas Demostradas
Big Data & Procesamiento Distribuido

âœ… ManipulaciÃ³n de DataFrames a gran escala (100K+ registros)
âœ… OptimizaciÃ³n de queries con particionamiento estratÃ©gico
âœ… Broadcast joins y cache management
âœ… Window functions para anÃ¡lisis complejos

Delta Lake & Data Lakehouse

âœ… ACID transactions en data lakes
âœ… Time Travel para auditorÃ­a y recuperaciÃ³n
âœ… OPTIMIZE y Z-ORDERING para performance
âœ… VACUUM para gestiÃ³n de storage
âœ… Change Data Feed (CDF) para pipelines incrementales

Streaming & Real-Time Processing

âœ… Structured Streaming con Apache Spark
âœ… DeduplicaciÃ³n con watermarks
âœ… Procesamiento continuo de datos IoT

Data Quality & Governance

âœ… Manejo avanzado de valores nulos
âœ… ValidaciÃ³n con expresiones regulares
âœ… Slowly Changing Dimensions (SCD Type 2)
âœ… Merge/Upsert operations


ğŸ—ï¸ Arquitectura TÃ©cnica
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATABRICKS WORKSPACE                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   NIVEL 1-4  â”‚â”€â”€â”€â–¶â”‚  NIVEL 5-8   â”‚â”€â”€â”€â–¶â”‚  NIVEL 9-13  â”‚ â”‚
â”‚  â”‚  Fundamentos â”‚    â”‚ Delta Lake   â”‚    â”‚   Advanced   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                    â”‚                    â”‚         â”‚
â”‚         â–¼                    â–¼                    â–¼         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Apache Spark 4.0 (Distributed Engine)     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                 â”‚
â”‚                           â–¼                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚          Delta Lake Storage Layer (DBFS)           â”‚    â”‚
â”‚  â”‚  â€¢ ACID Transactions  â€¢ Time Travel  â€¢ Optimize    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ’¼ Casos de Uso Implementados
1ï¸âƒ£ ETL Empresarial con Agregaciones Complejas
Procesamiento de 100K+ registros de ventas con agregaciones multi-nivel y window functions.
python# Ranking de salarios por departamento con Window Functions
ventana_dept = Window.partitionBy("departamento").orderBy(col("salario").desc())

df_ranking = df_grande.withColumn(
    "ranking_dept",
    rank().over(ventana_dept)
).withColumn(
    "row_number_dept",
    row_number().over(ventana_dept)
)

# Top 3 empleados mejor pagados por departamento
df_ranking.filter(col("ranking_dept") <= 3)\
    .select("departamento", "nombre", "salario", "ranking_dept")\
    .orderBy("departamento", "ranking_dept")
Resultado: Sistema de ranking dinÃ¡mico que escala a millones de registros sin degradaciÃ³n de performance.

2ï¸âƒ£ Data Lakehouse con Time Travel
ImplementaciÃ³n de auditorÃ­a completa con capacidad de rollback a cualquier versiÃ³n histÃ³rica.
python# Ver historial completo de versiones
delta_table.history().select(
    "version", "timestamp", "operation", "operationMetrics"
).show(truncate=False)

# Time Travel: Leer datos de hace 7 dÃ­as
df_version_anterior = spark.read.format("delta")\
    .option("versionAsOf", 5)\
    .load(ruta_delta_clientes)

# Restaurar a versiÃ³n anterior si detectamos error
delta_table.restoreToVersion(5)
Impacto: Capacidad de recuperaciÃ³n ante errores crÃ­ticos y cumplimiento de requisitos de auditorÃ­a regulatoria.

3ï¸âƒ£ OptimizaciÃ³n de Performance (10x Faster)
AplicaciÃ³n de tÃ©cnicas avanzadas de optimizaciÃ³n para queries de anÃ¡lisis.
python# ANTES: Query sin optimizar (tabla sin compactar)
# Archivos: 50+ pequeÃ±os fragmentos | Tiempo: ~15s

# OPTIMIZACIÃ“N 1: CompactaciÃ³n de archivos
delta_table.optimize().executeCompaction()

# OPTIMIZACIÃ“N 2: Z-Ordering en columnas frecuentemente filtradas
delta_table.optimize().executeZOrderBy("region", "producto")

# DESPUÃ‰S: 
# Archivos: 3 archivos optimizados | Tiempo: ~1.5s
# âœ… Mejora de 10x en performance
MÃ©trica: ReducciÃ³n del 90% en tiempo de ejecuciÃ³n de queries analÃ­ticas.

4ï¸âƒ£ Slowly Changing Dimensions (SCD Type 2)
ImplementaciÃ³n de versionado histÃ³rico completo para anÃ¡lisis temporal.
python# Mantener historial completo de cambios en clientes
# Registro antiguo: es_actual=False, fecha_fin=NOW()
# Registro nuevo: es_actual=True, fecha_inicio=NOW()

delta_table_clientes_scd.alias("destino").merge(
    df_cambios.alias("origen"),
    """destino.cliente_id = origen.cliente_id AND 
       destino.es_actual = true AND
       (destino.ciudad != origen.ciudad OR destino.email != origen.email)"""
).whenMatchedUpdate(
    set = {
        "es_actual": "false",
        "fecha_fin": "current_timestamp()"
    }
).execute()
Valor: Rastreo completo de cambios histÃ³ricos para anÃ¡lisis de tendencias y cumplimiento GDPR.

5ï¸âƒ£ Streaming Real-Time con DeduplicaciÃ³n
Pipeline de procesamiento continuo para datos IoT con garantÃ­as de exactly-once.
python# Stream de sensores con deduplicaciÃ³n automÃ¡tica
stream_deduplicado = spark.readStream\
    .format("json")\
    .schema(schema_sensores)\
    .load(ruta_streaming_input)\
    .withWatermark("event_time", "1 minute")\
    .dropDuplicates(["event_id", "user_id"])

# Escritura continua a Delta Lake
query = stream_deduplicado.writeStream\
    .format("delta")\
    .outputMode("append")\
    .option("checkpointLocation", checkpoint_path)\
    .trigger(processingTime="5 seconds")\
    .start(ruta_delta_output)
AplicaciÃ³n: Procesamiento de eventos IoT, telemetrÃ­a de aplicaciones, logs en tiempo real.

6ï¸âƒ£ Change Data Feed (CDC) para Pipelines Incrementales
Arquitectura de data warehouse con cargas incrementales eficientes.
python# Leer solo cambios desde Ãºltima carga (evita full scan)
df_changes = spark.read.format("delta")\
    .option("readChangeFeed", "true")\
    .option("startingVersion", ultima_version_procesada + 1)\
    .load(ruta_tabla_origen)

# Aplicar cambios al DW con MERGE inteligente
delta_table_dw.alias("destino").merge(
    df_changes_to_apply.alias("origen"),
    "destino.cliente_id = origen.cliente_id"
).whenMatchedDelete(
    condition = "origen._change_type = 'delete'"
).whenMatchedUpdate(
    condition = "origen._change_type = 'update_postimage'",
    set = {...}
).whenNotMatchedInsert(
    condition = "origen._change_type = 'insert'",
    values = {...}
).execute()
Beneficio: ReducciÃ³n del 95% en tiempo de procesamiento ETL al procesar solo deltas en lugar de full loads.

ğŸ“ˆ Resultados Cuantificables
MÃ©tricaAntesDespuÃ©sMejoraTiempo de Query AnalÃ­tica15s1.5s10x fasterArchivos en Storage50+ fragmentos3 optimizados-94% filesProcesamiento ETLFull load diarioIncremental CDC-95% tiempoData Recovery TimeN/A< 1 minTime TravelDeduplicaciÃ³n StreamingManualAutomÃ¡tica100% accuracy

ğŸ› ï¸ Stack TecnolÃ³gico
Procesamiento:

Apache Spark 4.0 (PySpark)
Distributed Computing (Cluster mode)

Storage:

Delta Lake 3.0 (ACID transactions)
Parquet (columnar format)
DBFS (Databricks File System)

OptimizaciÃ³n:

Z-Ordering & Data Skipping
Broadcast Joins
Partition Pruning
Adaptive Query Execution (AQE)

Streaming:

Structured Streaming
Watermarks & Event Time
Exactly-Once Semantics

Data Governance:

Change Data Feed (CDF)
Time Travel (versioning)
VACUUM (retention policies)


ğŸ“š Niveles de Aprendizaje
ğŸ¯ Nivel 1-2: Fundamentos (LÃ­neas 1-150)

DataFrames, selecciones, filtros, agregaciones
Joins (inner, left, broadcast)
Window functions y rankings

ğŸ¯ Nivel 3-4: Transformaciones Avanzadas (LÃ­neas 151-300)

Manejo de fechas y timestamps
Limpieza de datos y valores nulos
Expresiones regulares para validaciÃ³n

ğŸ¯ Nivel 5-6: OptimizaciÃ³n (LÃ­neas 301-450)

Particionamiento estratÃ©gico
Cache y persist
Broadcast joins para performance

ğŸ¯ Nivel 7-8: Delta Lake Fundamentals (LÃ­neas 451-650)

CreaciÃ³n de Delta Tables
Time Travel (versionAsOf, timestampAsOf)
RESTORE para rollback

ğŸ¯ Nivel 9-10: OptimizaciÃ³n Delta (LÃ­neas 651-850)

OPTIMIZE & compaction
Z-ORDERING para queries selectivas
VACUUM para gestiÃ³n de storage

ğŸ¯ Nivel 11: MERGE & Upserts (LÃ­neas 851-1050)

MERGE operations (INSERT/UPDATE/DELETE)
Slowly Changing Dimensions (SCD Type 2)
Conditional merges

ğŸ¯ Nivel 12: Streaming (LÃ­neas 1051-1250)

Structured Streaming con Delta Lake
DeduplicaciÃ³n con watermarks
Cargas incrementales

ğŸ¯ Nivel 13: Change Data Feed (LÃ­neas 1251-1450)

CDC para pipelines incrementales
Tracking de cambios por tipo
Data warehouse incremental con CDF


ğŸ“ Habilidades Transferibles a Empresas
Para Roles de Data Engineer:
âœ… DiseÃ±o de arquitecturas data lakehouse escalables
âœ… ImplementaciÃ³n de pipelines ETL/ELT con PySpark
âœ… OptimizaciÃ³n de costos en cloud (storage + compute)
âœ… GarantÃ­as ACID en data lakes
Para Roles de Analytics Engineer:
âœ… Modelado dimensional (SCD Type 2)
âœ… Queries analÃ­ticas de alto rendimiento
âœ… Data quality & governance
Para Roles de Data Architect:
âœ… Estrategias de particionamiento y indexaciÃ³n
âœ… Time Travel para compliance y auditorÃ­a
âœ… Streaming architecture para datos real-time
