{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78f3740e-2a88-4677-a3f9-de6b05884552",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83C\uDFAF NIVEL 1: FUNDAMENTOS BÁSICOS\n",
    "Ejercicio 1.1: Crear tu primer DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f45abcbb-dfc1-4785-a6bd-d999d0e4851c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de Spark: 4.0.0\n+---+------+---------+----+------------+-------+\n| id|nombre| apellido|edad|departamento|salario|\n+---+------+---------+----+------------+-------+\n|  1|  Juan|    Pérez|  30|      Ventas|   3000|\n|  2| María|   García|  25|          IT|   3500|\n|  3|Carlos|    López|  35|      Ventas|   3200|\n|  4|   Ana| Martínez|  28|          IT|   3800|\n|  5|  Luis|Rodríguez|  40|        RRHH|   3100|\n+---+------+---------+----+------------+-------+\n\nroot\n |-- id: long (nullable = true)\n |-- nombre: string (nullable = true)\n |-- apellido: string (nullable = true)\n |-- edad: long (nullable = true)\n |-- departamento: string (nullable = true)\n |-- salario: long (nullable = true)\n\nNúmero de filas: 5\nNúmero de columnas: 6\n"
     ]
    }
   ],
   "source": [
    "# Importar librerías necesarias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Verificar la sesión de Spark (en Databricks ya está creada como 'spark')\n",
    "print(f\"Versión de Spark: {spark.version}\")\n",
    "# Crear un DataFrame simple desde listas\n",
    "datos_empleados = [\n",
    "    (1, \"Juan\", \"Pérez\", 30, \"Ventas\", 3000),\n",
    "    (2, \"María\", \"García\", 25, \"IT\", 3500),\n",
    "    (3, \"Carlos\", \"López\", 35, \"Ventas\", 3200),\n",
    "    (4, \"Ana\", \"Martínez\", 28, \"IT\", 3800),\n",
    "    (5, \"Luis\", \"Rodríguez\", 40, \"RRHH\", 3100)\n",
    "]\n",
    "\n",
    "columnas = [\"id\", \"nombre\", \"apellido\", \"edad\", \"departamento\", \"salario\"]\n",
    "\n",
    "df_empleados = spark.createDataFrame(datos_empleados, columnas)\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "df_empleados.show()\n",
    "\n",
    "# Ver el esquema\n",
    "df_empleados.printSchema()\n",
    "\n",
    "# Información básica\n",
    "print(f\"Número de filas: {df_empleados.count()}\")\n",
    "print(f\"Número de columnas: {len(df_empleados.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44c3be4f-95f0-497f-85f8-bd5c59a7f82e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ejercicio 1.2: Operaciones Básicas de Selección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6079739a-099a-480b-903d-98a17571c612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-------+\n|nombre| apellido|salario|\n+------+---------+-------+\n|  Juan|    Pérez|   3000|\n| María|   García|   3500|\n|Carlos|    López|   3200|\n|   Ana| Martínez|   3800|\n|  Luis|Rodríguez|   3100|\n+------+---------+-------+\n\n+------+---------------+-------------+\n|nombre|salario_mensual|salario_anual|\n+------+---------------+-------------+\n|  Juan|           3000|        36000|\n| María|           3500|        42000|\n|Carlos|           3200|        38400|\n|   Ana|           3800|        45600|\n|  Luis|           3100|        37200|\n+------+---------------+-------------+\n\n+---+------+---------+----+------------+-------+\n| id|nombre| apellido|edad|departamento|salario|\n+---+------+---------+----+------------+-------+\n|  3|Carlos|    López|  35|      Ventas|   3200|\n|  5|  Luis|Rodríguez|  40|        RRHH|   3100|\n+---+------+---------+----+------------+-------+\n\n+---+------+--------+----+------------+-------+\n| id|nombre|apellido|edad|departamento|salario|\n+---+------+--------+----+------------+-------+\n|  4|   Ana|Martínez|  28|          IT|   3800|\n+---+------+--------+----+------------+-------+\n\n+---+------+---------+----+------------+-------+\n| id|nombre| apellido|edad|departamento|salario|\n+---+------+---------+----+------------+-------+\n|  4|   Ana| Martínez|  28|          IT|   3800|\n|  2| María|   García|  25|          IT|   3500|\n|  3|Carlos|    López|  35|      Ventas|   3200|\n|  5|  Luis|Rodríguez|  40|        RRHH|   3100|\n|  1|  Juan|    Pérez|  30|      Ventas|   3000|\n+---+------+---------+----+------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar columnas específicas\n",
    "df_empleados.select(\"nombre\", \"apellido\", \"salario\").show()\n",
    "\n",
    "# Seleccionar con alias\n",
    "df_empleados.select(\n",
    "    col(\"nombre\"),\n",
    "    col(\"salario\").alias(\"salario_mensual\"),\n",
    "    (col(\"salario\") * 12).alias(\"salario_anual\")\n",
    ").show()\n",
    "\n",
    "# Filtrar registros\n",
    "df_empleados.filter(col(\"edad\") > 30).show()\n",
    "\n",
    "# Múltiples condiciones\n",
    "df_empleados.filter(\n",
    "    (col(\"departamento\") == \"IT\") & (col(\"salario\") > 3500)\n",
    ").show()\n",
    "\n",
    "# Ordenar resultados\n",
    "df_empleados.orderBy(col(\"salario\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e1ead12-85dc-41f4-a80a-0c3578cf05a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ejercicio 1.3: Crear DataFrames con Esquemas Definidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56d539ad-9005-481d-adfe-f46d75b7b4f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = false)\n |-- nombre: string (nullable = false)\n |-- apellido: string (nullable = false)\n |-- edad: integer (nullable = true)\n |-- departamento: string (nullable = true)\n |-- salario: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Definir esquema explícitamente\n",
    "schema_empleados = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"nombre\", StringType(), False),\n",
    "    StructField(\"apellido\", StringType(), False),\n",
    "    StructField(\"edad\", IntegerType(), True),\n",
    "    StructField(\"departamento\", StringType(), True),\n",
    "    StructField(\"salario\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df_con_esquema = spark.createDataFrame(datos_empleados, schema_empleados)\n",
    "df_con_esquema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba52613b-5952-406d-9f8a-1e2ce039c870",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83C\uDFAF NIVEL 2: OPERACIONES CON DATAFRAMES\n",
    "Ejercicio 2.1: Agregaciones y Agrupaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af56dc19-ce5e-4d88-a63c-a03a5013c4e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset creado con 1000 registros\n+---+----------+------------+--------+----+-------+----------------+\n| id|    nombre|departamento|  ciudad|edad|salario|años_experiencia|\n+---+----------+------------+--------+----+-------+----------------+\n|  1| Empleado1|        RRHH|   Cusco|  47|5420.13|               9|\n|  2| Empleado2|      Ventas|    Lima|  52|4833.88|               6|\n|  3| Empleado3|        RRHH|   Cusco|  44|7047.38|               1|\n|  4| Empleado4|   Marketing|Trujillo|  32|5525.62|              10|\n|  5| Empleado5|   Marketing|   Cusco|  51|3291.58|               4|\n|  6| Empleado6|    Finanzas|Arequipa|  22|6054.55|               9|\n|  7| Empleado7|    Finanzas|Arequipa|  41|4650.14|               3|\n|  8| Empleado8|    Finanzas|Arequipa|  47|3956.11|               8|\n|  9| Empleado9|    Finanzas|   Cusco|  37|7054.85|              10|\n| 10|Empleado10|   Marketing|   Cusco|  50|5319.26|               9|\n+---+----------+------------+--------+----+-------+----------------+\nonly showing top 10 rows\n+-----------------+--------------+--------------+-------------------+\n| salario_promedio|salario_maximo|salario_minimo|masa_salarial_total|\n+-----------------+--------------+--------------+-------------------+\n|5198.518079999997|       7989.01|       2534.24|  5198518.079999997|\n+-----------------+--------------+--------------+-------------------+\n\n+------------+-------------+------------------+--------------+\n|departamento|num_empleados|  salario_promedio|salario_maximo|\n+------------+-------------+------------------+--------------+\n|      Ventas|          199| 5297.224974874371|       7974.08|\n|          IT|          208| 5205.062788461539|       7927.68|\n|        RRHH|          189|5194.0279894179885|       7963.06|\n|    Finanzas|          197|  5172.37395939086|       7989.01|\n|   Marketing|          207| 5126.030386473431|       7950.98|\n+------------+-------------+------------------+--------------+\n\n+------------+--------+---------+------------------+\n|departamento|  ciudad|empleados|  salario_promedio|\n+------------+--------+---------+------------------+\n|    Finanzas|Arequipa|       52|5063.2699999999995|\n|    Finanzas|   Cusco|       61| 5235.899344262295|\n|    Finanzas|    Lima|       46| 5304.022173913044|\n|    Finanzas|Trujillo|       38| 5060.335526315788|\n|          IT|Arequipa|       49| 5279.375510204083|\n|          IT|   Cusco|       52|5257.0115384615365|\n|          IT|    Lima|       58| 5188.326034482757|\n|          IT|Trujillo|       49| 5095.431632653061|\n|   Marketing|Arequipa|       51|5150.1001960784315|\n|   Marketing|   Cusco|       62| 5107.354999999999|\n|   Marketing|    Lima|       47|  4878.88914893617|\n|   Marketing|Trujillo|       47|5371.6889361702115|\n|        RRHH|Arequipa|       41| 5264.558048780488|\n|        RRHH|   Cusco|       51| 5073.065098039216|\n|        RRHH|    Lima|       47| 5197.659148936171|\n|        RRHH|Trujillo|       50|         5256.1622|\n|      Ventas|Arequipa|       47| 5132.010638297873|\n|      Ventas|   Cusco|       48|5654.8516666666665|\n|      Ventas|    Lima|       53| 4959.966603773587|\n|      Ventas|Trujillo|       51|  5463.37568627451|\n+------------+--------+---------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Crear dataset más grande para agregaciones\n",
    "from random import randint, choice, uniform\n",
    "import builtins\n",
    "departamentos = [\"Ventas\", \"IT\", \"RRHH\", \"Marketing\", \"Finanzas\"]\n",
    "ciudades = [\"Lima\", \"Arequipa\", \"Cusco\", \"Trujillo\"]\n",
    "\n",
    "datos_grandes = [\n",
    "    (\n",
    "        i,\n",
    "        f\"Empleado{i}\",\n",
    "        choice(departamentos),\n",
    "        choice(ciudades),\n",
    "        randint(22, 60),\n",
    "        builtins.round(uniform(2500, 8000), 2),\n",
    "        randint(1, 10)\n",
    "    )\n",
    "    for i in range(1, 1001)\n",
    "]\n",
    "\n",
    "columnas_grandes = [\"id\", \"nombre\", \"departamento\", \"ciudad\", \"edad\", \"salario\", \"años_experiencia\"]\n",
    "\n",
    "df_grande = spark.createDataFrame(datos_grandes, columnas_grandes)\n",
    "\n",
    "print(f\"Dataset creado con {df_grande.count()} registros\")\n",
    "df_grande.show(10)\n",
    "\n",
    "# Agregaciones básicas\n",
    "df_grande.select(\n",
    "    avg(\"salario\").alias(\"salario_promedio\"),\n",
    "    max(\"salario\").alias(\"salario_maximo\"),\n",
    "    min(\"salario\").alias(\"salario_minimo\"),\n",
    "    sum(\"salario\").alias(\"masa_salarial_total\")\n",
    ").show()\n",
    "\n",
    "# Agrupaciones\n",
    "df_grande.groupBy(\"departamento\").agg(\n",
    "    count(\"*\").alias(\"num_empleados\"),\n",
    "    avg(\"salario\").alias(\"salario_promedio\"),\n",
    "    max(\"salario\").alias(\"salario_maximo\")\n",
    ").orderBy(col(\"salario_promedio\").desc()).show()\n",
    "\n",
    "# Agrupaciones múltiples\n",
    "df_grande.groupBy(\"departamento\", \"ciudad\").agg(\n",
    "    count(\"*\").alias(\"empleados\"),\n",
    "    avg(\"salario\").alias(\"salario_promedio\")\n",
    ").orderBy(\"departamento\", \"ciudad\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9d15cee-4473-4765-8ab3-3d2ce44c1e3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ejercicio 2.2: Joins (Uniones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7197928-2091-41b1-8b1e-c0e0ed8ac9b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear tabla de bonos\n",
    "datos_bonos = [\n",
    "    (\"Ventas\", 500),\n",
    "    (\"IT\", 800),\n",
    "    (\"Marketing\", 600),\n",
    "    (\"Finanzas\", 700)\n",
    "]\n",
    "\n",
    "df_bonos = spark.createDataFrame(datos_bonos, [\"departamento\", \"bono_anual\"])\n",
    "\n",
    "# Inner Join\n",
    "df_con_bonos = df_grande.join(df_bonos, \"departamento\", \"inner\")\n",
    "df_con_bonos.select(\"nombre\", \"departamento\", \"salario\", \"bono_anual\").show(10)\n",
    "\n",
    "# Left Join (muestra RRHH aunque no tenga bono)\n",
    "df_left = df_grande.join(df_bonos, \"departamento\", \"left\")\n",
    "df_left.select(\"nombre\", \"departamento\", \"salario\", \"bono_anual\").show(10)\n",
    "\n",
    "# Calcular compensación total\n",
    "df_compensacion = df_con_bonos.withColumn(\n",
    "    \"compensacion_total\",\n",
    "    col(\"salario\") * 12 + col(\"bono_anual\")\n",
    ")\n",
    "\n",
    "df_compensacion.select(\n",
    "    \"nombre\", \"departamento\", \"salario\", \"bono_anual\", \"compensacion_total\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d17cd408-d8b8-4251-9975-422e6c66bd50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ejercicio 2.3: Window Functions (Funciones de Ventana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27c19d38-0d33-4dd2-9ac1-611046567cd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Ranking de salarios por departamento\n",
    "ventana_dept = Window.partitionBy(\"departamento\").orderBy(col(\"salario\").desc())\n",
    "\n",
    "df_ranking = df_grande.withColumn(\n",
    "    \"ranking_dept\",\n",
    "    rank().over(ventana_dept)\n",
    ").withColumn(\n",
    "    \"row_number_dept\",\n",
    "    row_number().over(ventana_dept)\n",
    ")\n",
    "\n",
    "df_ranking.filter(col(\"ranking_dept\") <= 3).select(\n",
    "    \"departamento\", \"nombre\", \"salario\", \"ranking_dept\"\n",
    ").orderBy(\"departamento\", \"ranking_dept\").show(20)\n",
    "\n",
    "# Calcular diferencia con el salario promedio del departamento\n",
    "ventana_dept_sin_orden = Window.partitionBy(\"departamento\")\n",
    "\n",
    "df_comparacion = df_grande.withColumn(\n",
    "    \"salario_promedio_dept\",\n",
    "    avg(\"salario\").over(ventana_dept_sin_orden)\n",
    ").withColumn(\n",
    "    \"diferencia_vs_promedio\",\n",
    "    col(\"salario\") - col(\"salario_promedio_dept\")\n",
    ")\n",
    "\n",
    "df_comparacion.select(\n",
    "    \"departamento\", \"nombre\", \"salario\", \n",
    "    \"salario_promedio_dept\", \"diferencia_vs_promedio\"\n",
    ").show(10)\n",
    "\n",
    "# Running total (suma acumulativa)\n",
    "ventana_running = Window.partitionBy(\"departamento\").orderBy(\"id\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "df_running = df_grande.withColumn(\n",
    "    \"suma_acumulativa_salarios\",\n",
    "    sum(\"salario\").over(ventana_running)\n",
    ")\n",
    "\n",
    "df_running.select(\"departamento\", \"id\", \"salario\", \"suma_acumulativa_salarios\").show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb401055-d0e4-4792-b5c8-4a7558faa148",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83C\uDFAF NIVEL 3: TRANSFORMACIONES AVANZADAS\n",
    "Ejercicio 3.1: Trabajar con Fechas y Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6b9d1ec-7c6e-4a58-b4b6-3ebf38bd9668",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from random import randint\n",
    "\n",
    "# Crear dataset con fechas\n",
    "fecha_inicio = datetime(2020, 1, 1)\n",
    "\n",
    "datos_transacciones = [\n",
    "    (\n",
    "        i,\n",
    "        f\"CLI{randint(1, 100)}\",\n",
    "        fecha_inicio + timedelta(days=randint(0, 1500)),\n",
    "        round(uniform(10, 5000), 2),\n",
    "        choice([\"Completada\", \"Pendiente\", \"Cancelada\"])\n",
    "    )\n",
    "    for i in range(1, 5001)\n",
    "]\n",
    "\n",
    "df_transacciones = spark.createDataFrame(\n",
    "    datos_transacciones,\n",
    "    [\"id_transaccion\", \"cliente_id\", \"fecha\", \"monto\", \"estado\"]\n",
    ")\n",
    "\n",
    "df_transacciones.show(10)\n",
    "\n",
    "# Extraer componentes de fecha\n",
    "df_fechas = df_transacciones.withColumn(\"año\", year(\"fecha\")) \\\n",
    "    .withColumn(\"mes\", month(\"fecha\")) \\\n",
    "    .withColumn(\"dia\", dayofmonth(\"fecha\")) \\\n",
    "    .withColumn(\"dia_semana\", dayofweek(\"fecha\")) \\\n",
    "    .withColumn(\"nombre_dia\", date_format(\"fecha\", \"EEEE\")) \\\n",
    "    .withColumn(\"trimestre\", quarter(\"fecha\"))\n",
    "\n",
    "df_fechas.select(\"fecha\", \"año\", \"mes\", \"dia\", \"nombre_dia\", \"trimestre\").show(10)\n",
    "\n",
    "# Análisis temporal\n",
    "df_transacciones.filter(col(\"estado\") == \"Completada\") \\\n",
    "    .withColumn(\"año\", year(\"fecha\")) \\\n",
    "    .withColumn(\"mes\", month(\"fecha\")) \\\n",
    "    .groupBy(\"año\", \"mes\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_transacciones\"),\n",
    "        sum(\"monto\").alias(\"ventas_totales\"),\n",
    "        avg(\"monto\").alias(\"ticket_promedio\")\n",
    "    ) \\\n",
    "    .orderBy(\"año\", \"mes\") \\\n",
    "    .show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0e4f8f0-de27-40d5-a53d-6de68c6ef77a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ejercicio 3.2: Manejo de Valores Nulos y Limpieza de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "330c7c53-4261-40fe-afbb-3b65182a6544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear dataset con valores nulos\n",
    "datos_con_nulos = [\n",
    "    (1, \"Juan\", 30, 3000, \"Lima\"),\n",
    "    (2, \"María\", None, 3500, \"Cusco\"),\n",
    "    (3, \"Carlos\", 35, None, \"Lima\"),\n",
    "    (4, None, 28, 3800, None),\n",
    "    (5, \"Luis\", 40, 3100, \"Arequipa\"),\n",
    "    (6, \"Ana\", None, None, \"Lima\")\n",
    "]\n",
    "\n",
    "df_nulos = spark.createDataFrame(\n",
    "    datos_con_nulos,\n",
    "    [\"id\", \"nombre\", \"edad\", \"salario\", \"ciudad\"]\n",
    ")\n",
    "\n",
    "print(\"Dataset original con nulos:\")\n",
    "df_nulos.show()\n",
    "\n",
    "# Contar nulos por columna\n",
    "from pyspark.sql.functions import col, count, when, isnan\n",
    "\n",
    "df_nulos.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in df_nulos.columns\n",
    "]).show()\n",
    "\n",
    "# Eliminar filas con cualquier nulo\n",
    "df_sin_nulos = df_nulos.dropna()\n",
    "print(f\"Filas después de dropna(): {df_sin_nulos.count()}\")\n",
    "\n",
    "# Eliminar filas solo si todas las columnas son nulas\n",
    "df_sin_todos_nulos = df_nulos.dropna(how='all')\n",
    "print(f\"Filas después de dropna(how='all'): {df_sin_todos_nulos.count()}\")\n",
    "\n",
    "# Rellenar nulos con valores específicos\n",
    "df_rellenado = df_nulos.fillna({\n",
    "    \"nombre\": \"Desconocido\",\n",
    "    \"edad\": 0,\n",
    "    \"salario\": 0.0,\n",
    "    \"ciudad\": \"No especificada\"\n",
    "})\n",
    "\n",
    "print(\"Dataset con nulos rellenados:\")\n",
    "df_rellenado.show()\n",
    "\n",
    "# Rellenar con el promedio (para columnas numéricas)\n",
    "promedio_salario = df_nulos.select(avg(\"salario\")).collect()[0][0]\n",
    "promedio_edad = df_nulos.select(avg(\"edad\")).collect()[0][0]\n",
    "\n",
    "df_con_promedios = df_nulos.fillna({\n",
    "    \"salario\": promedio_salario,\n",
    "    \"edad\": int(promedio_edad)\n",
    "})\n",
    "\n",
    "df_con_promedios.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd783657-4c87-4f11-bbcb-889cdf69f1a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ejercicio 3.3: Expresiones Regulares y Manipulación de Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73f1e789-2d38-4a5e-b08e-fa5678aa9eeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear dataset con textos\n",
    "datos_productos = [\n",
    "    (1, \"  iPhone 14 Pro - 256GB  \", \"Apple-2023-TECH\", \"tech@apple.com\"),\n",
    "    (2, \"Samsung Galaxy S23\", \"SAMSUNG_2023_mobile\", \"contact@samsung.co.kr\"),\n",
    "    (3, \"laptop  DELL XPS 15\", \"dell-2023-computers\", \"support@dell.com\"),\n",
    "    (4, \"AirPods Pro 2nd Gen\", \"apple-2023-audio\", \"SALES@APPLE.COM\"),\n",
    "    (5, \"Sony WH-1000XM5\", \"sony_2023_audio\", \"info@sony.jp\")\n",
    "]\n",
    "\n",
    "df_productos = spark.createDataFrame(\n",
    "    datos_productos,\n",
    "    [\"id\", \"nombre\", \"codigo\", \"email\"]\n",
    ")\n",
    "\n",
    "print(\"Dataset original:\")\n",
    "df_productos.show(truncate=False)\n",
    "\n",
    "# Limpiar espacios\n",
    "df_limpio = df_productos.withColumn(\n",
    "    \"nombre_limpio\",\n",
    "    trim(col(\"nombre\"))\n",
    ").withColumn(\n",
    "    \"nombre_upper\",\n",
    "    upper(col(\"nombre\"))\n",
    ").withColumn(\n",
    "    \"nombre_lower\",\n",
    "    lower(col(\"nombre\"))\n",
    ")\n",
    "\n",
    "df_limpio.select(\"nombre\", \"nombre_limpio\", \"nombre_upper\", \"nombre_lower\").show(truncate=False)\n",
    "\n",
    "# Extraer información con expresiones regulares\n",
    "df_extraido = df_productos.withColumn(\n",
    "    \"marca\",\n",
    "    regexp_extract(col(\"codigo\"), r\"^([a-zA-Z]+)\", 1)\n",
    ").withColumn(\n",
    "    \"año\",\n",
    "    regexp_extract(col(\"codigo\"), r\"(\\d{4})\", 1)\n",
    ").withColumn(\n",
    "    \"categoria\",\n",
    "    regexp_extract(col(\"codigo\"), r\"[_-]([a-zA-Z]+)$\", 1)\n",
    ")\n",
    "\n",
    "df_extraido.select(\"codigo\", \"marca\", \"año\", \"categoria\").show(truncate=False)\n",
    "\n",
    "# Validar emails\n",
    "df_validado = df_productos.withColumn(\n",
    "    \"email_valido\",\n",
    "    when(\n",
    "        col(\"email\").rlike(r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"),\n",
    "        \"Válido\"\n",
    "    ).otherwise(\"Inválido\")\n",
    ")\n",
    "\n",
    "df_validado.select(\"email\", \"email_valido\").show(truncate=False)\n",
    "\n",
    "# Reemplazar patrones\n",
    "df_normalizado = df_productos.withColumn(\n",
    "    \"codigo_normalizado\",\n",
    "    regexp_replace(col(\"codigo\"), r\"[_-]\", \" \")\n",
    ")\n",
    "\n",
    "df_normalizado.select(\"codigo\", \"codigo_normalizado\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efff8627-f38e-47a5-9af9-c93728f88c8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83C\uDFAF NIVEL 4: OPTIMIZACIÓN Y PERFORMANCE\n",
    "Ejercicio 4.1: Particionamiento y Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0d20663-6614-44f7-8623-8300f8eaf9fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear un dataset más grande\n",
    "datos_ventas = [ \n",
    "    (\n",
    "        i,\n",
    "        choice([\"2023\", \"2024\", \"2025\"]),\n",
    "        choice([\"Q1\", \"Q2\", \"Q3\", \"Q4\"]),\n",
    "        choice([\"Producto A\", \"Producto B\", \"Producto C\", \"Producto D\"]),\n",
    "        choice([\"Norte\", \"Sur\", \"Este\", \"Oeste\"]),\n",
    "        builtins.round(uniform(100, 10000), 2)\n",
    "    )\n",
    "    for i in range(1, 100001)\n",
    "]\n",
    "\n",
    "df_ventas = spark.createDataFrame(\n",
    "    datos_ventas,\n",
    "    [\"id\", \"año\", \"trimestre\", \"producto\", \"region\", \"ventas\"]\n",
    ")\n",
    "\n",
    "print(f\"Dataset de ventas creado: {df_ventas.count()} registros\")\n",
    "\n",
    "# Ver el plan de ejecución\n",
    "print(\"\\n=== Plan de ejecución sin optimizar ===\")\n",
    "df_ventas.filter(col(\"año\") == \"2024\").explain()\n",
    "\n",
    "# Hacer cache del DataFrame\n",
    "df_ventas.cache()\n",
    "df_ventas.count()  # Materializar el cache\n",
    "\n",
    "# Comparar performance\n",
    "import time\n",
    "\n",
    "# Sin cache (aunque ya hicimos cache, lo mostramos conceptualmente)\n",
    "start = time.time()\n",
    "result1 = df_ventas.filter(col(\"año\") == \"2024\").count()\n",
    "time1 = time.time() - start\n",
    "print(f\"Primera consulta: {result1} registros en {time1:.3f} segundos\")\n",
    "\n",
    "# Con cache (segunda ejecución es más rápida)\n",
    "start = time.time()\n",
    "result2 = df_ventas.filter(col(\"año\") == \"2024\").count()\n",
    "time2 = time.time() - start\n",
    "print(f\"Segunda consulta (con cache): {result2} registros en {time2:.3f} segundos\")\n",
    "\n",
    "# Liberar cache\n",
    "df_ventas.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5379cb06-36fa-4a03-8c96-2606290dee7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ejercicio 4.2: Broadcast Joins para Tablas Pequeñas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bd9025c-b4a4-4754-a052-80bc182a7a2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Tabla grande de transacciones\n",
    "datos_transacciones_grandes = [\n",
    "    (i, randint(1, 100), round(uniform(10, 1000), 2))\n",
    "    for i in range(1, 50001)\n",
    "]\n",
    "\n",
    "df_transacciones_grandes = spark.createDataFrame(\n",
    "    datos_transacciones_grandes,\n",
    "    [\"id_transaccion\", \"id_producto\", \"monto\"]\n",
    ")\n",
    "\n",
    "# Tabla pequeña de productos (candidata para broadcast)\n",
    "datos_productos_catalogo = [\n",
    "    (i, f\"Producto {i}\", choice([\"Electrónica\", \"Ropa\", \"Alimentos\", \"Hogar\"]))\n",
    "    for i in range(1, 101)\n",
    "]\n",
    "\n",
    "df_catalogo = spark.createDataFrame(\n",
    "    datos_productos_catalogo,\n",
    "    [\"id_producto\", \"nombre_producto\", \"categoria\"]\n",
    ")\n",
    "\n",
    "# Join normal\n",
    "print(\"=== Join Normal ===\")\n",
    "df_join_normal = df_transacciones_grandes.join(\n",
    "    df_catalogo,\n",
    "    \"id_producto\"\n",
    ")\n",
    "df_join_normal.explain()\n",
    "\n",
    "# Broadcast Join (más eficiente para tablas pequeñas)\n",
    "print(\"\\n=== Broadcast Join ===\")\n",
    "df_join_broadcast = df_transacciones_grandes.join(\n",
    "    broadcast(df_catalogo),\n",
    "    \"id_producto\"\n",
    ")\n",
    "df_join_broadcast.explain()\n",
    "\n",
    "# Verificar resultados\n",
    "df_join_broadcast.groupBy(\"categoria\").agg(\n",
    "    count(\"*\").alias(\"transacciones\"),\n",
    "    sum(\"monto\").alias(\"ventas_totales\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f72e830c-184e-4093-bbc8-4684ab1d9aa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ejercicio 4.3: Reparticionamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04f63df9-9bf0-49be-9a1e-f858b0e2ccf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ver particiones actuales\n",
    "print(f\"Número de particiones: {df_ventas.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Reparticionar por columna (útil para operaciones por grupo)\n",
    "df_reparticionado = df_ventas.repartition(8, \"año\", \"trimestre\")\n",
    "print(f\"Particiones después de repartition: {df_reparticionado.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Coalesce (reducir particiones sin shuffle completo)\n",
    "df_reducido = df_ventas.coalesce(2)\n",
    "print(f\"Particiones después de coalesce: {df_reducido.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Análisis por partición\n",
    "df_reparticionado.groupBy(\"año\", \"trimestre\").agg(\n",
    "    sum(\"ventas\").alias(\"ventas_totales\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3382b648-c0a1-4ec4-9d03-820423967339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83C\uDFAF NIVEL 6: TÉCNICAS AVANZADAS\n",
    "Ejercicio 6.1: Pivot y Unpivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4df2280d-62d1-466f-9623-c965d153ce9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear datos de ventas mensuales\n",
    "datos_ventas_mes = [\n",
    "    (\"Norte\", \"Enero\", 10000),\n",
    "    (\"Norte\", \"Febrero\", 12000),\n",
    "    (\"Norte\", \"Marzo\", 11000),\n",
    "    (\"Sur\", \"Enero\", 8000),\n",
    "    (\"Sur\", \"Febrero\", 9000),\n",
    "    (\"Sur\", \"Marzo\", 8500),\n",
    "    (\"Este\", \"Enero\", 15000),\n",
    "    (\"Este\", \"Febrero\", 16000),\n",
    "    (\"Este\", \"Marzo\", 17000),\n",
    "]\n",
    "\n",
    "df_ventas_mes = spark.createDataFrame(\n",
    "    datos_ventas_mes,\n",
    "    [\"region\", \"mes\", \"ventas\"]\n",
    ")\n",
    "\n",
    "print(\"Datos originales:\")\n",
    "df_ventas_mes.show()\n",
    "\n",
    "# PIVOT: Convertir filas en columnas\n",
    "df_pivot = df_ventas_mes.groupBy(\"region\").pivot(\"mes\").sum(\"ventas\")\n",
    "\n",
    "print(\"\\nDatos con PIVOT (meses como columnas):\")\n",
    "df_pivot.show()\n",
    "\n",
    "# UNPIVOT: Convertir columnas en filas\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df_unpivot = df_pivot.selectExpr(\n",
    "    \"region\",\n",
    "    \"stack(3, 'Enero', Enero, 'Febrero', Febrero, 'Marzo', Marzo) as (mes, ventas)\"\n",
    ")\n",
    "\n",
    "print(\"\\nDatos con UNPIVOT (volver al formato original):\")\n",
    "df_unpivot.show()\n",
    "\n",
    "# Pivot con múltiples agregaciones\n",
    "df_pivot_multiple = df_ventas_mes.groupBy(\"region\").pivot(\"mes\").agg(\n",
    "    sum(\"ventas\").alias(\"total\"),\n",
    "    count(\"*\").alias(\"transacciones\")\n",
    ")\n",
    "\n",
    "df_pivot_multiple.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd60e9fb-5e9a-4d63-810c-ba7e43f4d9cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83C\uDFAF NIVEL 7: DELTA LAKE - FUNDAMENTOS\n",
    "Ejercicio 7.1: Crear tu Primera Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c06411c9-6540-4fce-ac68-b38d1bf7d10a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Limpiar directorios anteriores si existen\n",
    "dbutils.fs.rm(\"/tmp/delta_tutorial\", True)\n",
    "\n",
    "# Crear datos iniciales\n",
    "from pyspark.sql.functions import *\n",
    "from random import randint, choice, uniform\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Dataset de clientes\n",
    "datos_clientes = [\n",
    "    (i, f\"Cliente_{i}\", choice([\"Lima\", \"Cusco\", \"Arequipa\", \"Trujillo\"]), \n",
    "     round(uniform(1000, 50000), 2), datetime(2024, 1, 1) + timedelta(days=randint(0, 300)))\n",
    "    for i in range(1, 1001)\n",
    "]\n",
    "\n",
    "df_clientes = spark.createDataFrame(\n",
    "    datos_clientes,\n",
    "    [\"cliente_id\", \"nombre\", \"ciudad\", \"valor_total\", \"fecha_registro\"]\n",
    ")\n",
    "\n",
    "print(\"Dataset inicial de clientes:\")\n",
    "df_clientes.show(10)\n",
    "\n",
    "# Escribir como Delta Table\n",
    "ruta_delta_clientes = \"/tmp/delta_tutorial/clientes\"\n",
    "\n",
    "df_clientes.write.format(\"delta\").mode(\"overwrite\").save(ruta_delta_clientes)\n",
    "\n",
    "print(f\"✓ Delta Table creada en: {ruta_delta_clientes}\")\n",
    "\n",
    "# Leer Delta Table\n",
    "df_delta_clientes = spark.read.format(\"delta\").load(ruta_delta_clientes)\n",
    "\n",
    "print(f\"Registros en Delta Table: {df_delta_clientes.count()}\")\n",
    "df_delta_clientes.show(5)\n",
    "\n",
    "# Ver el historial de la tabla (versiones)\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, ruta_delta_clientes)\n",
    "\n",
    "print(\"\\n=== HISTORIAL DE VERSIONES ===\")\n",
    "delta_table.history().select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28443e16-f8df-4a2c-8b96-5c29743f26a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ejercicio 7.2: Crear Delta Table Administrada (Managed Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88b25f81-e4da-4af3-81f7-dd03b802acfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear tabla administrada (en el metastore de Databricks)\n",
    "df_clientes.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"clientes_delta\")\n",
    "\n",
    "print(\"✓ Tabla administrada 'clientes_delta' creada\")\n",
    "\n",
    "# Leer desde tabla administrada\n",
    "df_managed = spark.table(\"clientes_delta\")\n",
    "df_managed.show(5)\n",
    "\n",
    "# Ver ubicación de la tabla administrada\n",
    "spark.sql(\"DESCRIBE EXTENDED clientes_delta\").filter(col(\"col_name\") == \"Location\").show(truncate=False)\n",
    "\n",
    "# Listar todas las tablas delta\n",
    "print(\"\\n=== TABLAS DELTA DISPONIBLES ===\")\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0855057a-887a-44df-afa9-3f1b3027efe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83C\uDFAF NIVEL 8: TIME TRAVEL\n",
    "Ejercicio 8.1: Time Travel Básico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f5d4cf9-fac8-4b00-8232-69abfdc866db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=== VERSIÓN 0 (INICIAL) ===\")\n",
    "df_delta_clientes.groupBy(\"ciudad\").count().show()\n",
    "\n",
    "# Realizar UPDATE (versión 1)\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, ruta_delta_clientes)\n",
    "\n",
    "delta_table.update(\n",
    "    condition = \"ciudad = 'Lima'\",\n",
    "    set = {\"valor_total\": \"valor_total * 1.10\"}  # Incremento 10% para Lima\n",
    ")\n",
    "\n",
    "print(\"\\n=== VERSIÓN 1 (DESPUÉS DE UPDATE) ===\")\n",
    "df_v1 = spark.read.format(\"delta\").load(ruta_delta_clientes)\n",
    "df_v1.filter(col(\"ciudad\") == \"Lima\").select(\"cliente_id\", \"nombre\", \"ciudad\", \"valor_total\").show(5)\n",
    "\n",
    "# Realizar DELETE (versión 2)\n",
    "delta_table.delete(\"valor_total < 2000\")\n",
    "\n",
    "print(\"\\n=== VERSIÓN 2 (DESPUÉS DE DELETE) ===\")\n",
    "df_v2 = spark.read.format(\"delta\").load(ruta_delta_clientes)\n",
    "print(f\"Registros después de DELETE: {df_v2.count()}\")\n",
    "\n",
    "# Insertar nuevos registros (versión 3)\n",
    "nuevos_clientes = [\n",
    "    (1001, \"Cliente_Nuevo_1\", \"Lima\", 25000.00, datetime.now()),\n",
    "    (1002, \"Cliente_Nuevo_2\", \"Cusco\", 30000.00, datetime.now()),\n",
    "]\n",
    "\n",
    "df_nuevos = spark.createDataFrame(\n",
    "    nuevos_clientes,\n",
    "    [\"cliente_id\", \"nombre\", \"ciudad\", \"valor_total\", \"fecha_registro\"]\n",
    ")\n",
    "\n",
    "df_nuevos.write.format(\"delta\").mode(\"append\").save(ruta_delta_clientes)\n",
    "\n",
    "print(\"\\n=== VERSIÓN 3 (DESPUÉS DE INSERT) ===\")\n",
    "df_v3 = spark.read.format(\"delta\").load(ruta_delta_clientes)\n",
    "print(f\"Total de registros: {df_v3.count()}\")\n",
    "\n",
    "# Ver todas las versiones en el historial\n",
    "print(\"\\n=== HISTORIAL COMPLETO ===\")\n",
    "delta_table.history().select(\n",
    "    \"version\", \"timestamp\", \"operation\", \n",
    "    col(\"operationMetrics.numOutputRows\").alias(\"rows_affected\")\n",
    ").show(truncate=False)\n",
    "\n",
    "# TIME TRAVEL: Leer versión específica por número\n",
    "print(\"\\n=== TIME TRAVEL: Leer Versión 0 ===\")\n",
    "df_version_0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(ruta_delta_clientes)\n",
    "print(f\"Registros en versión 0: {df_version_0.count()}\")\n",
    "df_version_0.groupBy(\"ciudad\").count().show()\n",
    "\n",
    "print(\"\\n=== TIME TRAVEL: Leer Versión 1 ===\")\n",
    "df_version_1 = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(ruta_delta_clientes)\n",
    "print(f\"Registros en versión 1: {df_version_1.count()}\")\n",
    "\n",
    "print(\"\\n=== TIME TRAVEL: Leer Versión 2 ===\")\n",
    "df_version_2 = spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(ruta_delta_clientes)\n",
    "print(f\"Registros en versión 2: {df_version_2.count()}\")\n",
    "\n",
    "# TIME TRAVEL: Leer por timestamp\n",
    "timestamp_v1 = delta_table.history().filter(col(\"version\") == 1).select(\"timestamp\").collect()[0][0]\n",
    "\n",
    "print(f\"\\n=== TIME TRAVEL: Leer por Timestamp {timestamp_v1} ===\")\n",
    "df_by_timestamp = spark.read.format(\"delta\").option(\"timestampAsOf\", timestamp_v1).load(ruta_delta_clientes)\n",
    "print(f\"Registros: {df_by_timestamp.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cdda183-7397-4eab-bb3d-55d8d3dd1e99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ejercicio 8.2: Restaurar Versiones Anteriores (RESTORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ca716ed-4d0a-42b9-8774-ea3b94104d64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ver estado actual\n",
    "print(\"=== ESTADO ACTUAL (Versión 3) ===\")\n",
    "df_actual = spark.read.format(\"delta\").load(ruta_delta_clientes)\n",
    "print(f\"Total registros: {df_actual.count()}\")\n",
    "\n",
    "# RESTORE: Volver a la versión 0\n",
    "print(\"\\n=== RESTAURANDO A VERSIÓN 0 ===\")\n",
    "delta_table.restoreToVersion(0)\n",
    "\n",
    "# Verificar restauración\n",
    "df_restaurado = spark.read.format(\"delta\").load(ruta_delta_clientes)\n",
    "print(f\"Registros después de RESTORE: {df_restaurado.count()}\")\n",
    "\n",
    "# Ver historial después del restore\n",
    "print(\"\\n=== HISTORIAL DESPUÉS DE RESTORE ===\")\n",
    "delta_table.history().select(\"version\", \"timestamp\", \"operation\").show(10, truncate=False)\n",
    "\n",
    "# Nota: RESTORE crea una nueva versión (versión 4 en este caso)\n",
    "print(\"\\n=== Leer Versión Actual (después de RESTORE) ===\")\n",
    "df_version_actual = spark.read.format(\"delta\").load(ruta_delta_clientes)\n",
    "df_version_actual.groupBy(\"ciudad\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ad5143c-a1bc-44c6-9275-a35e6c8c60cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83C\uDFAF NIVEL 9: OPTIMIZACIÓN DELTA LAKE\n",
    "Ejercicio 9.1: OPTIMIZE - Compactación de Archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7f029f2-8d1e-4422-b7cf-55eca96e8e0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear tabla con muchos archivos pequeños\n",
    "print(\"=== CREANDO TABLA CON ARCHIVOS PEQUEÑOS ===\")\n",
    "ruta_delta_transacciones = \"/tmp/delta_tutorial/transacciones\"\n",
    "\n",
    "# Escribir en múltiples lotes pequeños (simula escrituras incrementales)\n",
    "for i in range(20):\n",
    "    datos_batch = [\n",
    "        (\n",
    "            j + (i * 100),\n",
    "            randint(1, 100),\n",
    "            datetime(2024, 10, 1) + timedelta(hours=i),\n",
    "            round(uniform(10, 1000), 2),\n",
    "            choice([\"COMPLETADA\", \"PENDIENTE\", \"CANCELADA\"])\n",
    "        )\n",
    "        for j in range(1, 101)\n",
    "    ]\n",
    "    \n",
    "    df_batch = spark.createDataFrame(\n",
    "        datos_batch,\n",
    "        [\"transaccion_id\", \"cliente_id\", \"fecha\", \"monto\", \"estado\"]\n",
    "    )\n",
    "    \n",
    "    df_batch.write.format(\"delta\").mode(\"append\").save(ruta_delta_transacciones)\n",
    "\n",
    "print(\"✓ Datos escritos en 20 lotes\")\n",
    "\n",
    "# Ver archivos antes de optimizar\n",
    "files_antes = dbutils.fs.ls(ruta_delta_transacciones)\n",
    "parquet_files_antes = [f for f in files_antes if f.name.endswith('.parquet')]\n",
    "print(f\"\\nArchivos .parquet ANTES de OPTIMIZE: {len(parquet_files_antes)}\")\n",
    "\n",
    "# Ver tamaño de archivos\n",
    "for f in parquet_files_antes[:5]:\n",
    "    print(f\"  {f.name}: {f.size / 1024:.2f} KB\")\n",
    "\n",
    "# OPTIMIZE: Compactar archivos pequeños\n",
    "print(\"\\n=== EJECUTANDO OPTIMIZE ===\")\n",
    "delta_table_trans = DeltaTable.forPath(spark, ruta_delta_transacciones)\n",
    "\n",
    "result = delta_table_trans.optimize().executeCompaction()\n",
    "print(\"✓ OPTIMIZE completado\")\n",
    "\n",
    "# Mostrar métricas de optimización\n",
    "result.show(truncate=False)\n",
    "\n",
    "# Ver archivos después de optimizar\n",
    "files_despues = dbutils.fs.ls(ruta_delta_transacciones)\n",
    "parquet_files_despues = [f for f in files_despues if f.name.endswith('.parquet')]\n",
    "print(f\"\\nArchivos .parquet DESPUÉS de OPTIMIZE: {len(parquet_files_despues)}\")\n",
    "\n",
    "for f in parquet_files_despues[:5]:\n",
    "    print(f\"  {f.name}: {f.size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e42130dc-9373-4a56-b246-ba5dc3c8f212",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ejercicio 9.2: Z-ORDERING - Optimización de Consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "805e05fd-671a-4f05-9935-c36dc18cc34c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear tabla para demostrar Z-ORDERING\n",
    "ruta_delta_ventas = \"/tmp/delta_tutorial/ventas_zorder\"\n",
    "\n",
    "datos_ventas_grande = [\n",
    "    (\n",
    "        i,\n",
    "        choice([\"Norte\", \"Sur\", \"Este\", \"Oeste\", \"Centro\"]),\n",
    "        choice([\"Producto_A\", \"Producto_B\", \"Producto_C\", \"Producto_D\", \"Producto_E\"]),\n",
    "        datetime(2024, 1, 1) + timedelta(days=randint(0, 300)),\n",
    "        round(uniform(10, 5000), 2)\n",
    "    )\n",
    "    for i in range(1, 50001)\n",
    "]\n",
    "\n",
    "df_ventas_grande = spark.createDataFrame(\n",
    "    datos_ventas_grande,\n",
    "    [\"venta_id\", \"region\", \"producto\", \"fecha\", \"monto\"]\n",
    ")\n",
    "\n",
    "df_ventas_grande.write.format(\"delta\").mode(\"overwrite\").save(ruta_delta_ventas)\n",
    "\n",
    "print(\"✓ Tabla de ventas creada\")\n",
    "\n",
    "delta_table_ventas = DeltaTable.forPath(spark, ruta_delta_ventas)\n",
    "\n",
    "# Consulta SIN Z-ORDERING\n",
    "print(\"\\n=== CONSULTA SIN Z-ORDERING ===\")\n",
    "spark.read.format(\"delta\").load(ruta_delta_ventas) \\\n",
    "    .filter((col(\"region\") == \"Norte\") & (col(\"producto\") == \"Producto_A\")) \\\n",
    "    .explain()\n",
    "\n",
    "# Aplicar Z-ORDERING en columnas frecuentemente filtradas\n",
    "print(\"\\n=== APLICANDO Z-ORDERING en 'region' y 'producto' ===\")\n",
    "result_zorder = delta_table_ventas.optimize().executeZOrderBy(\"region\", \"producto\")\n",
    "result_zorder.show(truncate=False)\n",
    "\n",
    "print(\"✓ Z-ORDERING completado\")\n",
    "\n",
    "# Consulta CON Z-ORDERING\n",
    "print(\"\\n=== CONSULTA CON Z-ORDERING ===\")\n",
    "spark.read.format(\"delta\").load(ruta_delta_ventas) \\\n",
    "    .filter((col(\"region\") == \"Norte\") & (col(\"producto\") == \"Producto_A\")) \\\n",
    "    .explain()\n",
    "\n",
    "# Demostrar beneficio con estadísticas\n",
    "print(\"\\n=== COMPARACIÓN ===\")\n",
    "df_zorder = spark.read.format(\"delta\").load(ruta_delta_ventas)\n",
    "result = df_zorder.filter((col(\"region\") == \"Norte\") & (col(\"producto\") == \"Producto_A\")).count()\n",
    "print(f\"Registros encontrados: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38d317d9-0efa-4176-8845-c86b1ebfc040",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ejercicio 9.3: Data Skipping Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2b69f5e-7951-4a00-ba8e-21db04b44f42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ver estadísticas de la tabla\n",
    "print(\"=== ESTADÍSTICAS DE LA TABLA ===\")\n",
    "\n",
    "# Describir detalles de la tabla\n",
    "spark.sql(f\"DESCRIBE DETAIL delta.`{ruta_delta_ventas}`\").show(truncate=False)\n",
    "\n",
    "# Ver historial con métricas\n",
    "delta_table_ventas.history().select(\n",
    "    \"version\", \"timestamp\", \"operation\",\n",
    "    col(\"operationMetrics.numFiles\").alias(\"num_files\"),\n",
    "    col(\"operationMetrics.numOutputRows\").alias(\"output_rows\"),\n",
    "    col(\"operationMetrics.numOutputBytes\").alias(\"output_bytes\")\n",
    ").show(truncate=False)\n",
    "\n",
    "# Habilitar estadísticas extendidas (si están disponibles)\n",
    "spark.sql(f\"\"\"\n",
    "    ANALYZE TABLE delta.`{ruta_delta_ventas}` \n",
    "    COMPUTE STATISTICS FOR COLUMNS region, producto, fecha\n",
    "\"\"\")\n",
    "\n",
    "print(\"✓ Estadísticas computadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8be4cc6c-9660-4501-84e8-67b151be3187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83C\uDFAF NIVEL 10: VACUUM - LIMPIEZA DE ARCHIVOS\n",
    "Ejercicio 10.1: VACUUM Básico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af38ff77-33db-46fd-9281-d3909c6059de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ver archivos actuales\n",
    "print(\"=== ESTADO ANTES DE VACUUM ===\")\n",
    "all_files = dbutils.fs.ls(ruta_delta_clientes)\n",
    "print(f\"Total de archivos en la carpeta: {len(all_files)}\")\n",
    "\n",
    "# Ver historial\n",
    "print(\"\\nHistorial de versiones:\")\n",
    "delta_table.history().select(\"version\", \"timestamp\", \"operation\").show(truncate=False)\n",
    "\n",
    "# VACUUM DRY RUN: Ver qué se eliminaría sin eliminar realmente\n",
    "print(\"\\n=== VACUUM DRY RUN ===\")\n",
    "result_dry = spark.sql(f\"VACUUM delta.`{ruta_delta_clientes}` RETAIN 0 HOURS DRY RUN\")\n",
    "result_dry.show(truncate=False)\n",
    "\n",
    "# Deshabilitar retención de 7 días (SOLO PARA DEMO - NO HACER EN PRODUCCIÓN)\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "\n",
    "print(\"\\n=== EJECUTANDO VACUUM (eliminando archivos antiguos) ===\")\n",
    "try:\n",
    "    delta_table.vacuum(0)  # 0 horas de retención (solo para demo)\n",
    "    print(\"✓ VACUUM completado\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Ver archivos después de VACUUM\n",
    "all_files_after = dbutils.fs.ls(ruta_delta_clientes)\n",
    "print(f\"\\nTotal de archivos después de VACUUM: {len(all_files_after)}\")\n",
    "\n",
    "# Intentar leer versiones antiguas (fallarán después de VACUUM)\n",
    "print(\"\\n=== INTENTANDO LEER VERSIÓN ANTIGUA ===\")\n",
    "try:\n",
    "    df_old = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(ruta_delta_clientes)\n",
    "    print(f\"Versión 1 disponible: {df_old.count()} registros\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error al leer versión antigua: {str(e)[:100]}\")\n",
    "\n",
    "# Restaurar configuración\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b2497ed-6379-4eae-a1f2-e08a7e54ed0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ejercicio 10.2: VACUUM con Retención Configurable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b20b8b8-470f-4847-87a6-28aff1e18729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear nueva tabla para demostrar retención\n",
    "ruta_delta_productos = \"/tmp/delta_tutorial/productos\"\n",
    "\n",
    "datos_productos = [\n",
    "    (i, f\"Producto_{i}\", round(uniform(10, 1000), 2), randint(0, 1000))\n",
    "    for i in range(1, 501)\n",
    "]\n",
    "\n",
    "df_productos = spark.createDataFrame(\n",
    "    datos_productos,\n",
    "    [\"producto_id\", \"nombre\", \"precio\", \"stock\"]\n",
    ")\n",
    "\n",
    "df_productos.write.format(\"delta\").mode(\"overwrite\").save(ruta_delta_productos)\n",
    "\n",
    "delta_table_productos = DeltaTable.forPath(spark, ruta_delta_productos)\n",
    "\n",
    "# Hacer varios cambios\n",
    "for i in range(5):\n",
    "    delta_table_productos.update(\n",
    "        condition = f\"producto_id <= {(i+1)*100}\",\n",
    "        set = {\"precio\": \"precio * 1.05\"}\n",
    "    )\n",
    "\n",
    "print(\"✓ 5 actualizaciones realizadas\")\n",
    "\n",
    "# Ver historial\n",
    "print(\"\\n=== HISTORIAL ===\")\n",
    "delta_table_productos.history().select(\"version\", \"timestamp\", \"operation\").show(truncate=False)\n",
    "\n",
    "# VACUUM con retención de 168 horas (7 días) - valor recomendado en producción\n",
    "print(\"\\n=== VACUUM con retención de 168 horas (7 días) ===\")\n",
    "delta_table_productos.vacuum(168)\n",
    "\n",
    "print(\"✓ VACUUM completado\")\n",
    "print(\"  Archivos más antiguos que 7 días eliminados\")\n",
    "print(\"  Time Travel disponible para versiones de los últimos 7 días\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b60a2f1-6d6f-46ff-8c2b-33234a7fde16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83C\uDFAF NIVEL 11: MERGE - UPSERTS\n",
    "Ejercicio 11.1: MERGE Básico (INSERT + UPDATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02a599e7-1899-40ab-83b0-130f07cb8856",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear tabla destino\n",
    "ruta_delta_inventario = \"/tmp/delta_tutorial/inventario\"\n",
    "\n",
    "datos_inventario_inicial = [\n",
    "    (1, \"Laptop Dell\", 10, 1200.00, datetime(2024, 10, 1)),\n",
    "    (2, \"Mouse Logitech\", 50, 25.00, datetime(2024, 10, 1)),\n",
    "    (3, \"Teclado Mecánico\", 30, 80.00, datetime(2024, 10, 1)),\n",
    "    (4, \"Monitor Samsung\", 15, 300.00, datetime(2024, 10, 1)),\n",
    "    (5, \"Webcam HD\", 20, 60.00, datetime(2024, 10, 1)),\n",
    "]\n",
    "\n",
    "df_inventario = spark.createDataFrame(\n",
    "    datos_inventario_inicial,\n",
    "    [\"producto_id\", \"nombre\", \"cantidad\", \"precio\", \"ultima_actualizacion\"]\n",
    ")\n",
    "\n",
    "df_inventario.write.format(\"delta\").mode(\"overwrite\").save(ruta_delta_inventario)\n",
    "\n",
    "print(\"=== INVENTARIO INICIAL ===\")\n",
    "spark.read.format(\"delta\").load(ruta_delta_inventario).show()\n",
    "\n",
    "# Crear datos de actualización (algunos existen, otros son nuevos)\n",
    "datos_actualizacion = [\n",
    "    (2, \"Mouse Logitech\", 45, 25.00, datetime(2024, 10, 23)),  # UPDATE: cantidad cambia\n",
    "    (4, \"Monitor Samsung\", 18, 280.00, datetime(2024, 10, 23)),  # UPDATE: cantidad y precio\n",
    "    (6, \"Auriculares Sony\", 25, 150.00, datetime(2024, 10, 23)),  # INSERT: nuevo producto\n",
    "    (7, \"SSD 1TB\", 40, 120.00, datetime(2024, 10, 23)),  # INSERT: nuevo producto\n",
    "]\n",
    "\n",
    "df_actualizacion = spark.createDataFrame(\n",
    "    datos_actualizacion,\n",
    "    [\"producto_id\", \"nombre\", \"cantidad\", \"precio\", \"ultima_actualizacion\"]\n",
    ")\n",
    "\n",
    "print(\"\\n=== DATOS DE ACTUALIZACIÓN ===\")\n",
    "df_actualizacion.show()\n",
    "\n",
    "# MERGE: Combinar datos (UPSERT)\n",
    "delta_table_inventario = DeltaTable.forPath(spark, ruta_delta_inventario)\n",
    "\n",
    "print(\"\\n=== EJECUTANDO MERGE ===\")\n",
    "delta_table_inventario.alias(\"destino\").merge(\n",
    "    df_actualizacion.alias(\"origen\"),\n",
    "    \"destino.producto_id = origen.producto_id\"\n",
    ").whenMatchedUpdate(\n",
    "    set = {\n",
    "        \"cantidad\": \"origen.cantidad\",\n",
    "        \"precio\": \"origen.precio\",\n",
    "        \"ultima_actualizacion\": \"origen.ultima_actualizacion\"\n",
    "    }\n",
    ").whenNotMatchedInsert(\n",
    "    values = {\n",
    "        \"producto_id\": \"origen.producto_id\",\n",
    "        \"nombre\": \"origen.nombre\",\n",
    "        \"cantidad\": \"origen.cantidad\",\n",
    "        \"precio\": \"origen.precio\",\n",
    "        \"ultima_actualizacion\": \"origen.ultima_actualizacion\"\n",
    "    }\n",
    ").execute()\n",
    "\n",
    "print(\"✓ MERGE completado\")\n",
    "\n",
    "# Ver resultado\n",
    "print(\"\\n=== INVENTARIO DESPUÉS DE MERGE ===\")\n",
    "df_resultado = spark.read.format(\"delta\").load(ruta_delta_inventario)\n",
    "df_resultado.orderBy(\"producto_id\").show()\n",
    "\n",
    "# Ver métricas del merge\n",
    "print(\"\\n=== HISTORIAL DEL MERGE ===\")\n",
    "delta_table_inventario.history(1).select(\n",
    "    \"version\", \"operation\", \"operationMetrics\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ab232a3-c377-42c9-ae43-f457119b3d4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ejercicio 11.2: MERGE Condicional con DELETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25042523-cd1e-42c1-8a66-3b32112c133c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear tabla de pedidos\n",
    "ruta_delta_pedidos = \"/tmp/delta_tutorial/pedidos\"\n",
    "\n",
    "datos_pedidos = [\n",
    "    (1, 101, \"PENDIENTE\", 500.00, datetime(2024, 10, 1)),\n",
    "    (2, 102, \"ENVIADO\", 750.00, datetime(2024, 10, 2)),\n",
    "    (3, 103, \"PENDIENTE\", 300.00, datetime(2024, 10, 3)),\n",
    "    (4, 104, \"COMPLETADO\", 1200.00, datetime(2024, 10, 4)),\n",
    "    (5, 105, \"PENDIENTE\", 450.00, datetime(2024, 10, 5)),\n",
    "]\n",
    "\n",
    "df_pedidos = spark.createDataFrame(\n",
    "    datos_pedidos,\n",
    "    [\"pedido_id\", \"cliente_id\", \"estado\", \"total\", \"fecha\"]\n",
    ")\n",
    "\n",
    "df_pedidos.write.format(\"delta\").mode(\"overwrite\").save(ruta_delta_pedidos)\n",
    "\n",
    "print(\"=== PEDIDOS INICIALES ===\")\n",
    "spark.read.format(\"delta\").load(ruta_delta_pedidos).show()\n",
    "\n",
    "# Actualizaciones: cambiar estados y cancelar algunos pedidos\n",
    "datos_actualizacion_pedidos = [\n",
    "    (1, 101, \"ENVIADO\", 500.00, datetime(2024, 10, 23)),  # PENDIENTE -> ENVIADO\n",
    "    (3, 103, \"CANCELADO\", 300.00, datetime(2024, 10, 23)),  # PENDIENTE -> CANCELADO (eliminar)\n",
    "    (5, 105, \"ENVIADO\", 450.00, datetime(2024, 10, 23)),  # PENDIENTE -> ENVIADO\n",
    "    (6, 106, \"PENDIENTE\", 890.00, datetime(2024, 10, 23)),  # Nuevo pedido\n",
    "]\n",
    "\n",
    "df_actualizacion_pedidos = spark.createDataFrame(\n",
    "    datos_actualizacion_pedidos,\n",
    "    [\"pedido_id\", \"cliente_id\", \"estado\", \"total\", \"fecha\"]\n",
    ")\n",
    "\n",
    "print(\"\\n=== ACTUALIZACIONES DE PEDIDOS ===\")\n",
    "df_actualizacion_pedidos.show()\n",
    "\n",
    "# MERGE con DELETE condicional\n",
    "delta_table_pedidos = DeltaTable.forPath(spark, ruta_delta_pedidos)\n",
    "\n",
    "print(\"\\n=== EJECUTANDO MERGE CON DELETE ===\")\n",
    "delta_table_pedidos.alias(\"destino\").merge(\n",
    "    df_actualizacion_pedidos.alias(\"origen\"),\n",
    "    \"destino.pedido_id = origen.pedido_id\"\n",
    ").whenMatchedDelete(\n",
    "    condition = \"origen.estado = 'CANCELADO'\"  # Eliminar pedidos cancelados\n",
    ").whenMatchedUpdate(\n",
    "    condition = \"origen.estado != 'CANCELADO'\",\n",
    "    set = {\n",
    "        \"estado\": \"origen.estado\",\n",
    "        \"fecha\": \"origen.fecha\"\n",
    "    }\n",
    ").whenNotMatchedInsert(\n",
    "    values = {\n",
    "        \"pedido_id\": \"origen.pedido_id\",\n",
    "        \"cliente_id\": \"origen.cliente_id\",\n",
    "        \"estado\": \"origen.estado\",\n",
    "        \"total\": \"origen.total\",\n",
    "        \"fecha\": \"origen.fecha\"\n",
    "    }\n",
    ").execute()\n",
    "\n",
    "print(\"✓ MERGE con DELETE completado\")\n",
    "\n",
    "# Ver resultado\n",
    "print(\"\\n=== PEDIDOS DESPUÉS DE MERGE ===\")\n",
    "df_resultado_pedidos = spark.read.format(\"delta\").load(ruta_delta_pedidos)\n",
    "df_resultado_pedidos.orderBy(\"pedido_id\").show()\n",
    "\n",
    "# Ver métricas\n",
    "print(\"\\n=== MÉTRICAS DEL MERGE ===\")\n",
    "delta_table_pedidos.history(1).select(\n",
    "    \"version\", \"operation\", \n",
    "    col(\"operationMetrics.numTargetRowsInserted\").alias(\"insertados\"),\n",
    "    col(\"operationMetrics.numTargetRowsUpdated\").alias(\"actualizados\"),\n",
    "    col(\"operationMetrics.numTargetRowsDeleted\").alias(\"eliminados\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1a5051b-b953-4f58-9369-2a19506a6deb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ejercicio 11.3: MERGE para Slowly Changing Dimension (SCD Type 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09ca1787-90d0-4cd6-a246-6949dba104d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Implementar SCD Type 2: mantener historial de cambios\n",
    "ruta_delta_clientes_scd = \"/tmp/delta_tutorial/clientes_scd\"\n",
    "\n",
    "# Tabla inicial con clientes (incluye columnas de versionado)\n",
    "datos_clientes_scd = [\n",
    "    (1, \"Juan Pérez\", \"Lima\", \"juan@email.com\", True, datetime(2024, 1, 1), None),\n",
    "    (2, \"María García\", \"Cusco\", \"maria@email.com\", True, datetime(2024, 1, 1), None),\n",
    "    (3, \"Carlos López\", \"Arequipa\", \"carlos@email.com\", True, datetime(2024, 1, 1), None),\n",
    "]\n",
    "\n",
    "df_clientes_scd = spark.createDataFrame(\n",
    "    datos_clientes_scd,\n",
    "    [\"cliente_id\", \"nombre\", \"ciudad\", \"email\", \"es_actual\", \"fecha_inicio\", \"fecha_fin\"]\n",
    ")\n",
    "\n",
    "df_clientes_scd.write.format(\"delta\").mode(\"overwrite\").save(ruta_delta_clientes_scd)\n",
    "\n",
    "print(\"=== CLIENTES INICIALES (SCD Type 2) ===\")\n",
    "spark.read.format(\"delta\").load(ruta_delta_clientes_scd).show(truncate=False)\n",
    "\n",
    "# Cambios: Juan cambió de ciudad, María cambió email, nuevo cliente\n",
    "datos_cambios = [\n",
    "    (1, \"Juan Pérez\", \"Trujillo\", \"juan@email.com\"),  # Cambió ciudad\n",
    "    (2, \"María García\", \"Cusco\", \"maria.garcia@newemail.com\"),  # Cambió email\n",
    "    (4, \"Ana Martínez\", \"Lima\", \"ana@email.com\"),  # Cliente nuevo\n",
    "]\n",
    "\n",
    "df_cambios = spark.createDataFrame(\n",
    "    datos_cambios,\n",
    "    [\"cliente_id\", \"nombre\", \"ciudad\", \"email\"]\n",
    ")\n",
    "\n",
    "print(\"\\n=== CAMBIOS A APLICAR ===\")\n",
    "df_cambios.show(truncate=False)\n",
    "\n",
    "# Preparar datos para SCD Type 2\n",
    "# 1. Marcar registros antiguos como no actuales\n",
    "# 2. Insertar nuevos registros con la información actualizada\n",
    "\n",
    "df_cambios_preparados = df_cambios.withColumn(\"es_actual\", lit(True)) \\\n",
    "    .withColumn(\"fecha_inicio\", lit(datetime(2024, 10, 23))) \\\n",
    "    .withColumn(\"fecha_fin\", lit(None).cast(\"timestamp\"))\n",
    "\n",
    "delta_table_clientes_scd = DeltaTable.forPath(spark, ruta_delta_clientes_scd)\n",
    "\n",
    "# Primero: Cerrar registros antiguos que cambiaron\n",
    "print(\"\\n=== PASO 1: Cerrando registros antiguos ===\")\n",
    "delta_table_clientes_scd.alias(\"destino\").merge(\n",
    "    df_cambios.alias(\"origen\"),\n",
    "    \"\"\"destino.cliente_id = origen.cliente_id AND \n",
    "       destino.es_actual = true AND\n",
    "       (destino.ciudad != origen.ciudad OR destino.email != origen.email)\"\"\"\n",
    ").whenMatchedUpdate(\n",
    "    set = {\n",
    "        \"es_actual\": \"false\",\n",
    "        \"fecha_fin\": \"current_timestamp()\"\n",
    "    }\n",
    ").execute()\n",
    "\n",
    "# Segundo: Insertar nuevos registros (actualizados y nuevos clientes)\n",
    "print(\"=== PASO 2: Insertando nuevos registros ===\")\n",
    "delta_table_clientes_scd.alias(\"destino\").merge(\n",
    "    df_cambios_preparados.alias(\"origen\"),\n",
    "    \"destino.cliente_id = origen.cliente_id AND destino.es_actual = true\"\n",
    ").whenNotMatchedInsert(\n",
    "    values = {\n",
    "        \"cliente_id\": \"origen.cliente_id\",\n",
    "        \"nombre\": \"origen.nombre\",\n",
    "        \"ciudad\": \"origen.ciudad\",\n",
    "        \"email\": \"origen.email\",\n",
    "        \"es_actual\": \"origen.es_actual\",\n",
    "        \"fecha_inicio\": \"origen.fecha_inicio\",\n",
    "        \"fecha_fin\": \"origen.fecha_fin\"\n",
    "    }\n",
    ").execute()\n",
    "\n",
    "print(\"✓ SCD Type 2 aplicado\")\n",
    "\n",
    "# Ver resultado: historial completo\n",
    "print(\"\\n=== HISTORIAL COMPLETO DE CLIENTES (SCD Type 2) ===\")\n",
    "df_scd_resultado = spark.read.format(\"delta\").load(ruta_delta_clientes_scd)\n",
    "df_scd_resultado.orderBy(\"cliente_id\", \"fecha_inicio\").show(truncate=False)\n",
    "\n",
    "# Ver solo clientes actuales\n",
    "print(\"\\n=== SOLO CLIENTES ACTUALES ===\")\n",
    "df_scd_resultado.filter(col(\"es_actual\") == True).show(truncate=False)\n",
    "\n",
    "# Ver historial de un cliente específico\n",
    "print(\"\\n=== HISTORIAL DEL CLIENTE 1 (Juan) ===\")\n",
    "df_scd_resultado.filter(col(\"cliente_id\") == 1).orderBy(\"fecha_inicio\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "881fa089-616c-47a0-bf59-725ca993214c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83C\uDFAF NIVEL 12: CARGA INCREMENTAL Y STREAMING\n",
    "Ejercicio 12.1: Carga Incremental con MERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f90fd449-d1bc-4b0f-bc81-789adfc34b77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simular carga incremental diaria\n",
    "ruta_delta_ventas_diarias = \"/tmp/delta_tutorial/ventas_diarias\"\n",
    "\n",
    "# Carga inicial\n",
    "datos_dia_1 = [\n",
    "    (1, 101, datetime(2024, 10, 21), 500.00, \"COMPLETADA\"),\n",
    "    (2, 102, datetime(2024, 10, 21), 750.00, \"COMPLETADA\"),\n",
    "    (3, 103, datetime(2024, 10, 21), 300.00, \"COMPLETADA\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "885f75c9-f5a5-49b9-bdc9-92e3dac68fd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_dia_1 = spark.createDataFrame(\n",
    "    datos_dia_1,\n",
    "    [\"venta_id\", \"cliente_id\", \"fecha\", \"monto\", \"estado\"]\n",
    ")\n",
    "\n",
    "df_dia_1.write.format(\"delta\").mode(\"overwrite\").save(ruta_delta_ventas_diarias)\n",
    "\n",
    "print(\"=== DÍA 1: CARGA INICIAL ===\")\n",
    "spark.read.format(\"delta\").load(ruta_delta_ventas_diarias).show()\n",
    "\n",
    "# Simular carga incremental día 2\n",
    "datos_dia_2 = [\n",
    "    (2, 102, datetime(2024, 10, 21), 800.00, \"COMPLETADA\"),  # Actualización de venta 2\n",
    "    (4, 104, datetime(2024, 10, 22), 1200.00, \"COMPLETADA\"),  # Nueva venta\n",
    "    (5, 105, datetime(2024, 10, 22), 450.00, \"PENDIENTE\"),  # Nueva venta\n",
    "]\n",
    "\n",
    "df_dia_2 = spark.createDataFrame(\n",
    "    datos_dia_2,\n",
    "    [\"venta_id\", \"cliente_id\", \"fecha\", \"monto\", \"estado\"]\n",
    ")\n",
    "\n",
    "print(\"\\n=== DÍA 2: DATOS INCREMENTALES ===\")\n",
    "df_dia_2.show()\n",
    "\n",
    "# MERGE incremental\n",
    "delta_table_ventas_diarias = DeltaTable.forPath(spark, ruta_delta_ventas_diarias)\n",
    "\n",
    "delta_table_ventas_diarias.alias(\"destino\").merge(\n",
    "    df_dia_2.alias(\"origen\"),\n",
    "    \"destino.venta_id = origen.venta_id\"\n",
    ").whenMatchedUpdate(\n",
    "    set = {\n",
    "        \"monto\": \"origen.monto\",\n",
    "        \"estado\": \"origen.estado\",\n",
    "        \"fecha\": \"origen.fecha\"\n",
    "    }\n",
    ").whenNotMatchedInsert(\n",
    "    values = {\n",
    "        \"venta_id\": \"origen.venta_id\",\n",
    "        \"cliente_id\": \"origen.cliente_id\",\n",
    "        \"fecha\": \"origen.fecha\",\n",
    "        \"monto\": \"origen.monto\",\n",
    "        \"estado\": \"origen.estado\"\n",
    "    }\n",
    ").execute()\n",
    "\n",
    "print(\"\\n=== DESPUÉS DE CARGA DÍA 2 ===\")\n",
    "spark.read.format(\"delta\").load(ruta_delta_ventas_diarias).orderBy(\"venta_id\").show()\n",
    "\n",
    "# Simular carga incremental día 3\n",
    "datos_dia_3 = [\n",
    "    (5, 105, datetime(2024, 10, 22), 450.00, \"COMPLETADA\"),  # Actualización: PENDIENTE -> COMPLETADA\n",
    "    (6, 106, datetime(2024, 10, 23), 890.00, \"COMPLETADA\"),  # Nueva venta\n",
    "    (7, 107, datetime(2024, 10, 23), 650.00, \"COMPLETADA\"),  # Nueva venta\n",
    "]\n",
    "\n",
    "df_dia_3 = spark.createDataFrame(\n",
    "    datos_dia_3,\n",
    "    [\"venta_id\", \"cliente_id\", \"fecha\", \"monto\", \"estado\"]\n",
    ")\n",
    "\n",
    "print(\"\\n=== DÍA 3: DATOS INCREMENTALES ===\")\n",
    "df_dia_3.show()\n",
    "\n",
    "delta_table_ventas_diarias.alias(\"destino\").merge(\n",
    "    df_dia_3.alias(\"origen\"),\n",
    "    \"destino.venta_id = origen.venta_id\"\n",
    ").whenMatchedUpdate(\n",
    "    set = {\n",
    "        \"monto\": \"origen.monto\",\n",
    "        \"estado\": \"origen.estado\",\n",
    "        \"fecha\": \"origen.fecha\"\n",
    "    }\n",
    ").whenNotMatchedInsert(\n",
    "    values = {\n",
    "        \"venta_id\": \"origen.venta_id\",\n",
    "        \"cliente_id\": \"origen.cliente_id\",\n",
    "        \"fecha\": \"origen.fecha\",\n",
    "        \"monto\": \"origen.monto\",\n",
    "        \"estado\": \"origen.estado\"\n",
    "    }\n",
    ").execute()\n",
    "\n",
    "print(\"\\n=== DESPUÉS DE CARGA DÍA 3 ===\")\n",
    "spark.read.format(\"delta\").load(ruta_delta_ventas_diarias).orderBy(\"venta_id\").show()\n",
    "\n",
    "# Ver métricas de todas las cargas\n",
    "print(\"\\n=== HISTORIAL DE CARGAS INCREMENTALES ===\")\n",
    "delta_table_ventas_diarias.history().select(\n",
    "    \"version\", \"timestamp\", \"operation\",\n",
    "    col(\"operationMetrics.numTargetRowsInserted\").alias(\"insertados\"),\n",
    "    col(\"operationMetrics.numTargetRowsUpdated\").alias(\"actualizados\"),\n",
    "    col(\"operationMetrics.numOutputRows\").alias(\"total_rows\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "caf51e82-c56a-439c-8e28-2cd7d25dc712",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ejercicio 12.2: Streaming con Delta Lake (Structured Streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d34802be-c7c0-465a-8176-88242ea3d902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear carpeta para archivos de entrada (streaming source)\n",
    "ruta_streaming_input = \"/tmp/delta_tutorial/streaming_input\"\n",
    "ruta_streaming_checkpoint = \"/tmp/delta_tutorial/streaming_checkpoint\"\n",
    "ruta_delta_streaming_output = \"/tmp/delta_tutorial/streaming_output\"\n",
    "\n",
    "# Limpiar directorios previos\n",
    "dbutils.fs.rm(ruta_streaming_input, True)\n",
    "dbutils.fs.rm(ruta_streaming_checkpoint, True)\n",
    "dbutils.fs.rm(ruta_delta_streaming_output, True)\n",
    "\n",
    "# Crear directorio de entrada\n",
    "dbutils.fs.mkdirs(ruta_streaming_input)\n",
    "\n",
    "print(\"✓ Directorios de streaming creados\")\n",
    "\n",
    "# Escribir primer batch de datos\n",
    "datos_batch_1 = [\n",
    "    (1, \"sensor_001\", datetime(2024, 10, 23, 10, 0, 0), 25.5, 60.2),\n",
    "    (2, \"sensor_002\", datetime(2024, 10, 23, 10, 0, 0), 26.1, 58.5),\n",
    "    (3, \"sensor_003\", datetime(2024, 10, 23, 10, 0, 0), 24.8, 62.1),\n",
    "]\n",
    "\n",
    "df_batch_1 = spark.createDataFrame(\n",
    "    datos_batch_1,\n",
    "    [\"id\", \"sensor_id\", \"timestamp\", \"temperatura\", \"humedad\"]\n",
    ")\n",
    "\n",
    "df_batch_1.write.format(\"json\").mode(\"overwrite\").save(f\"{ruta_streaming_input}/batch_1\")\n",
    "\n",
    "print(\"✓ Batch 1 escrito\")\n",
    "\n",
    "# Configurar streaming reader\n",
    "schema_sensores = \"id INT, sensor_id STRING, timestamp TIMESTAMP, temperatura DOUBLE, humedad DOUBLE\"\n",
    "\n",
    "stream_reader = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(schema_sensores) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(ruta_streaming_input)\n",
    "\n",
    "print(\"✓ Stream reader configurado\")\n",
    "\n",
    "# Escribir a Delta Lake con streaming\n",
    "print(\"\\n=== INICIANDO STREAMING A DELTA LAKE ===\")\n",
    "\n",
    "query = stream_reader.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", ruta_streaming_checkpoint) \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .start(ruta_delta_streaming_output)\n",
    "\n",
    "print(\"✓ Streaming query iniciado\")\n",
    "\n",
    "# Esperar a que procese el primer batch\n",
    "import time\n",
    "time.sleep(10)\n",
    "\n",
    "# Ver datos procesados\n",
    "print(\"\\n=== DATOS DESPUÉS DEL BATCH 1 ===\")\n",
    "df_streaming_result = spark.read.format(\"delta\").load(ruta_delta_streaming_output)\n",
    "df_streaming_result.show()\n",
    "\n",
    "# Escribir segundo batch\n",
    "datos_batch_2 = [\n",
    "    (4, \"sensor_001\", datetime(2024, 10, 23, 10, 5, 0), 25.8, 59.8),\n",
    "    (5, \"sensor_002\", datetime(2024, 10, 23, 10, 5, 0), 26.5, 57.9),\n",
    "    (6, \"sensor_004\", datetime(2024, 10, 23, 10, 5, 0), 23.9, 63.5),\n",
    "]\n",
    "\n",
    "df_batch_2 = spark.createDataFrame(\n",
    "    datos_batch_2,\n",
    "    [\"id\", \"sensor_id\", \"timestamp\", \"temperatura\", \"humedad\"]\n",
    ")\n",
    "\n",
    "df_batch_2.write.format(\"json\").mode(\"overwrite\").save(f\"{ruta_streaming_input}/batch_2\")\n",
    "\n",
    "print(\"\\n✓ Batch 2 escrito\")\n",
    "\n",
    "# Esperar procesamiento\n",
    "time.sleep(10)\n",
    "\n",
    "# Ver datos actualizados\n",
    "print(\"\\n=== DATOS DESPUÉS DEL BATCH 2 ===\")\n",
    "df_streaming_result = spark.read.format(\"delta\").load(ruta_delta_streaming_output)\n",
    "df_streaming_result.orderBy(\"timestamp\").show()\n",
    "\n",
    "# Ver estadísticas del streaming\n",
    "print(\"\\n=== ESTADÍSTICAS DE STREAMING ===\")\n",
    "print(f\"Status: {query.status}\")\n",
    "print(f\"Query ID: {query.id}\")\n",
    "\n",
    "# Detener el query\n",
    "query.stop()\n",
    "print(\"\\n✓ Streaming query detenido\")\n",
    "\n",
    "# Ver historial de la tabla delta generada por streaming\n",
    "delta_table_streaming = DeltaTable.forPath(spark, ruta_delta_streaming_output)\n",
    "print(\"\\n=== HISTORIAL DE LA TABLA ===\")\n",
    "delta_table_streaming.history().select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef2b4d52-199b-499c-8a8d-fb3d5ef99049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ejercicio 12.3: Deduplicación en Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3540f86-1888-4e8d-8fac-9d8aa1f62bb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Streaming con deduplicación usando watermark\n",
    "ruta_streaming_dedup_input = \"/tmp/delta_tutorial/streaming_dedup_input\"\n",
    "ruta_streaming_dedup_checkpoint = \"/tmp/delta_tutorial/streaming_dedup_checkpoint\"\n",
    "ruta_delta_dedup_output = \"/tmp/delta_tutorial/streaming_dedup_output\"\n",
    "\n",
    "# Limpiar\n",
    "dbutils.fs.rm(ruta_streaming_dedup_input, True)\n",
    "dbutils.fs.rm(ruta_streaming_dedup_checkpoint, True)\n",
    "dbutils.fs.rm(ruta_delta_dedup_output, True)\n",
    "dbutils.fs.mkdirs(ruta_streaming_dedup_input)\n",
    "\n",
    "# Batch 1 con datos (algunos duplicados)\n",
    "datos_batch_dup_1 = [\n",
    "    (1, \"user_001\", \"login\", datetime(2024, 10, 23, 10, 0, 0)),\n",
    "    (1, \"user_001\", \"login\", datetime(2024, 10, 23, 10, 0, 0)),  # Duplicado\n",
    "    (2, \"user_002\", \"purchase\", datetime(2024, 10, 23, 10, 1, 0)),\n",
    "    (3, \"user_003\", \"logout\", datetime(2024, 10, 23, 10, 2, 0)),\n",
    "]\n",
    "\n",
    "df_batch_dup_1 = spark.createDataFrame(\n",
    "    datos_batch_dup_1,\n",
    "    [\"event_id\", \"user_id\", \"event_type\", \"event_time\"]\n",
    ")\n",
    "\n",
    "df_batch_dup_1.write.format(\"json\").mode(\"overwrite\").save(f\"{ruta_streaming_dedup_input}/batch_1\")\n",
    "\n",
    "print(\"✓ Batch con duplicados escrito\")\n",
    "\n",
    "# Configurar streaming con deduplicación\n",
    "schema_eventos = \"event_id INT, user_id STRING, event_type STRING, event_time TIMESTAMP\"\n",
    "\n",
    "stream_dedup = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(schema_eventos) \\\n",
    "    .load(ruta_streaming_dedup_input)\n",
    "\n",
    "# Aplicar watermark y deduplicación\n",
    "stream_deduplicado = stream_dedup \\\n",
    "    .withWatermark(\"event_time\", \"1 minute\") \\\n",
    "    .dropDuplicates([\"event_id\", \"user_id\"])\n",
    "\n",
    "print(\"✓ Deduplicación configurada\")\n",
    "\n",
    "# Escribir a Delta\n",
    "query_dedup = stream_deduplicado.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", ruta_streaming_dedup_checkpoint) \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .start(ruta_delta_dedup_output)\n",
    "\n",
    "print(\"✓ Streaming con deduplicación iniciado\")\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "# Ver resultado (debe tener 3 registros, no 4)\n",
    "print(\"\\n=== DATOS DEDUPLICADOS ===\")\n",
    "df_dedup_result = spark.read.format(\"delta\").load(ruta_delta_dedup_output)\n",
    "print(f\"Total de registros (esperado: 3): {df_dedup_result.count()}\")\n",
    "df_dedup_result.orderBy(\"event_time\").show()\n",
    "\n",
    "# Detener\n",
    "query_dedup.stop()\n",
    "print(\"✓ Query detenido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38ad8bea-901a-4c47-836a-b7615a3dbf58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83C\uDFAF NIVEL 13: CHANGE DATA FEED (CDF)\n",
    "Ejercicio 13.1: Habilitar y Usar Change Data Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2272f18-a4e7-4ca3-9d61-ec079e0fb64d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear tabla con Change Data Feed habilitado\n",
    "ruta_delta_cdf = \"/tmp/delta_tutorial/clientes_cdf\"\n",
    "\n",
    "# Limpiar\n",
    "dbutils.fs.rm(ruta_delta_cdf, True)\n",
    "\n",
    "# Datos iniciales\n",
    "datos_clientes_cdf = [\n",
    "    (1, \"Juan\", \"Lima\", 5000.00, \"ACTIVO\"),\n",
    "    (2, \"María\", \"Cusco\", 7500.00, \"ACTIVO\"),\n",
    "    (3, \"Carlos\", \"Arequipa\", 3200.00, \"ACTIVO\"),\n",
    "    (4, \"Ana\", \"Trujillo\", 9100.00, \"ACTIVO\"),\n",
    "    (5, \"Luis\", \"Lima\", 4300.00, \"ACTIVO\"),\n",
    "]\n",
    "\n",
    "df_clientes_cdf = spark.createDataFrame(\n",
    "    datos_clientes_cdf,\n",
    "    [\"cliente_id\", \"nombre\", \"ciudad\", \"saldo\", \"estado\"]\n",
    ")\n",
    "\n",
    "# Escribir con CDF habilitado\n",
    "df_clientes_cdf.write.format(\"delta\") \\\n",
    "    .option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(ruta_delta_cdf)\n",
    "\n",
    "print(\"✓ Tabla con Change Data Feed creada\")\n",
    "\n",
    "# Verificar que CDF está habilitado\n",
    "print(\"\\n=== PROPIEDADES DE LA TABLA ===\")\n",
    "spark.sql(f\"DESCRIBE DETAIL delta.`{ruta_delta_cdf}`\").select(\"format\", \"location\", \"properties\").show(truncate=False)\n",
    "\n",
    "# Ver datos iniciales (Versión 0)\n",
    "print(\"\\n=== VERSIÓN 0: DATOS INICIALES ===\")\n",
    "df_v0 = spark.read.format(\"delta\").load(ruta_delta_cdf)\n",
    "df_v0.show()\n",
    "\n",
    "# Operación 1: UPDATE (Versión 1)\n",
    "delta_table_cdf = DeltaTable.forPath(spark, ruta_delta_cdf)\n",
    "\n",
    "delta_table_cdf.update(\n",
    "    condition = \"ciudad = 'Lima'\",\n",
    "    set = {\"saldo\": \"saldo * 1.10\"}\n",
    ")\n",
    "\n",
    "print(\"\\n=== VERSIÓN 1: DESPUÉS DE UPDATE ===\")\n",
    "df_v1 = spark.read.format(\"delta\").load(ruta_delta_cdf)\n",
    "df_v1.show()\n",
    "\n",
    "# Operación 2: INSERT (Versión 2)\n",
    "datos_nuevos = [\n",
    "    (6, \"Pedro\", \"Piura\", 6500.00, \"ACTIVO\"),\n",
    "    (7, \"Sofia\", \"Ica\", 5200.00, \"ACTIVO\"),\n",
    "]\n",
    "\n",
    "df_nuevos = spark.createDataFrame(\n",
    "    datos_nuevos,\n",
    "    [\"cliente_id\", \"nombre\", \"ciudad\", \"saldo\", \"estado\"]\n",
    ")\n",
    "\n",
    "df_nuevos.write.format(\"delta\").mode(\"append\").save(ruta_delta_cdf)\n",
    "\n",
    "print(\"\\n=== VERSIÓN 2: DESPUÉS DE INSERT ===\")\n",
    "df_v2 = spark.read.format(\"delta\").load(ruta_delta_cdf)\n",
    "print(f\"Total registros: {df_v2.count()}\")\n",
    "\n",
    "# Operación 3: DELETE (Versión 3)\n",
    "delta_table_cdf.delete(\"saldo < 4000\")\n",
    "\n",
    "print(\"\\n=== VERSIÓN 3: DESPUÉS DE DELETE ===\")\n",
    "df_v3 = spark.read.format(\"delta\").load(ruta_delta_cdf)\n",
    "print(f\"Total registros: {df_v3.count()}\")\n",
    "df_v3.show()\n",
    "\n",
    "# LEER CHANGE DATA FEED: Ver todos los cambios\n",
    "print(\"\\n=== CHANGE DATA FEED: TODOS LOS CAMBIOS ===\")\n",
    "df_changes_all = spark.read.format(\"delta\") \\\n",
    "    .option(\"readChangeFeed\", \"true\") \\\n",
    "    .option(\"startingVersion\", 0) \\\n",
    "    .load(ruta_delta_cdf)\n",
    "\n",
    "df_changes_all.select(\n",
    "    \"cliente_id\", \"nombre\", \"ciudad\", \"saldo\", \"estado\",\n",
    "    \"_change_type\", \"_commit_version\", \"_commit_timestamp\"\n",
    ").orderBy(\"_commit_version\", \"cliente_id\").show(50, truncate=False)\n",
    "\n",
    "print(\"\\n=== TIPOS DE CAMBIOS ===\")\n",
    "df_changes_all.groupBy(\"_change_type\", \"_commit_version\").count().orderBy(\"_commit_version\").show()\n",
    "\n",
    "# Ver cambios entre versiones específicas\n",
    "print(\"\\n=== CAMBIOS ENTRE VERSIÓN 1 Y 2 ===\")\n",
    "df_changes_1_2 = spark.read.format(\"delta\") \\\n",
    "    .option(\"readChangeFeed\", \"true\") \\\n",
    "    .option(\"startingVersion\", 1) \\\n",
    "    .option(\"endingVersion\", 2) \\\n",
    "    .load(ruta_delta_cdf)\n",
    "\n",
    "df_changes_1_2.select(\n",
    "    \"cliente_id\", \"nombre\", \"_change_type\", \"_commit_version\"\n",
    ").show()\n",
    "\n",
    "# Ver solo inserciones\n",
    "print(\"\\n=== SOLO INSERCIONES ===\")\n",
    "df_changes_all.filter(col(\"_change_type\") == \"insert\").select(\n",
    "    \"cliente_id\", \"nombre\", \"ciudad\", \"saldo\", \"_commit_version\"\n",
    ").show()\n",
    "\n",
    "# Ver solo actualizaciones (muestra before y after)\n",
    "print(\"\\n=== SOLO ACTUALIZACIONES ===\")\n",
    "df_changes_all.filter(col(\"_change_type\").isin([\"update_preimage\", \"update_postimage\"])).select(\n",
    "    \"cliente_id\", \"nombre\", \"saldo\", \"_change_type\", \"_commit_version\"\n",
    ").orderBy(\"cliente_id\", \"_change_type\").show()\n",
    "\n",
    "# Ver solo eliminaciones\n",
    "print(\"\\n=== SOLO ELIMINACIONES ===\")\n",
    "df_changes_all.filter(col(\"_change_type\") == \"delete\").select(\n",
    "    \"cliente_id\", \"nombre\", \"ciudad\", \"saldo\", \"_commit_version\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d558fa0-2f3f-46bf-9e77-d52ceecae7d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ejercicio 13.2: CDC con Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5d8e63d-3e6f-4d5d-9617-28b23b45c45f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leer cambios por rango de tiempo\n",
    "print(\"\\n=== HISTORIAL DE VERSIONES CON TIMESTAMPS ===\")\n",
    "delta_table_cdf.history().select(\"version\", \"timestamp\", \"operation\").show(truncate=False)\n",
    "\n",
    "# Obtener timestamps\n",
    "timestamp_v1 = delta_table_cdf.history().filter(col(\"version\") == 1).select(\"timestamp\").collect()[0][0]\n",
    "timestamp_v3 = delta_table_cdf.history().filter(col(\"version\") == 3).select(\"timestamp\").collect()[0][0]\n",
    "\n",
    "print(f\"\\nTimestamp Versión 1: {timestamp_v1}\")\n",
    "print(f\"Timestamp Versión 3: {timestamp_v3}\")\n",
    "\n",
    "# Leer cambios por timestamp\n",
    "print(\"\\n=== CAMBIOS DESDE VERSIÓN 1 (POR TIMESTAMP) ===\")\n",
    "df_changes_by_time = spark.read.format(\"delta\") \\\n",
    "    .option(\"readChangeFeed\", \"true\") \\\n",
    "    .option(\"startingTimestamp\", timestamp_v1.strftime(\"%Y-%m-%d %H:%M:%S\")) \\\n",
    "    .load(ruta_delta_cdf)\n",
    "\n",
    "df_changes_by_time.select(\n",
    "    \"cliente_id\", \"nombre\", \"_change_type\", \"_commit_version\", \"_commit_timestamp\"\n",
    ").orderBy(\"_commit_version\", \"cliente_id\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c01168b9-d63c-4562-8076-7e736e595771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ejercicio 13.3: CDF para Data Pipeline Incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9ae91d0-4270-48ad-84c1-b3274f7e094e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Caso de uso: Procesar solo cambios para actualizar data warehouse\n",
    "print(\"\\n=== PIPELINE INCREMENTAL CON CDF ===\")\n",
    "\n",
    "# Simular tabla destino (data warehouse)\n",
    "ruta_delta_dw = \"/tmp/delta_tutorial/clientes_dw\"\n",
    "dbutils.fs.rm(ruta_delta_dw, True)\n",
    "\n",
    "# Primera carga completa (full load)\n",
    "print(\"\\n--- CARGA INICIAL COMPLETA ---\")\n",
    "df_initial = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(ruta_delta_cdf)\n",
    "df_initial.write.format(\"delta\").mode(\"overwrite\").save(ruta_delta_dw)\n",
    "\n",
    "print(f\"Carga inicial completada: {df_initial.count()} registros\")\n",
    "\n",
    "# Guardar última versión procesada\n",
    "ultima_version_procesada = 0\n",
    "\n",
    "# Función para procesar cambios incrementales\n",
    "def procesar_cambios_incrementales(version_inicial, version_final):\n",
    "    print(f\"\\n--- PROCESANDO CAMBIOS: Versión {version_inicial} -> {version_final} ---\")\n",
    "    \n",
    "    # Leer cambios\n",
    "    df_changes = spark.read.format(\"delta\") \\\n",
    "        .option(\"readChangeFeed\", \"true\") \\\n",
    "        .option(\"startingVersion\", version_inicial + 1) \\\n",
    "        .option(\"endingVersion\", version_final) \\\n",
    "        .load(ruta_delta_cdf)\n",
    "    \n",
    "    # Contar cambios por tipo\n",
    "    print(\"Cambios detectados:\")\n",
    "    df_changes.groupBy(\"_change_type\").count().show()\n",
    "    \n",
    "    # Preparar datos para MERGE en el DW\n",
    "    # Para updates: usar solo update_postimage (estado final)\n",
    "    # Para inserts: usar las filas insert\n",
    "    # Para deletes: usar las filas delete\n",
    "    \n",
    "    df_changes_to_apply = df_changes.filter(\n",
    "        (col(\"_change_type\") == \"insert\") |\n",
    "        (col(\"_change_type\") == \"update_postimage\") |\n",
    "        (col(\"_change_type\") == \"delete\")\n",
    "    )\n",
    "    \n",
    "    if df_changes_to_apply.count() > 0:\n",
    "        # Aplicar cambios al DW usando MERGE\n",
    "        delta_table_dw = DeltaTable.forPath(spark, ruta_delta_dw)\n",
    "        \n",
    "        delta_table_dw.alias(\"destino\").merge(\n",
    "            df_changes_to_apply.alias(\"origen\"),\n",
    "            \"destino.cliente_id = origen.cliente_id\"\n",
    "        ).whenMatchedDelete(\n",
    "            condition = \"origen._change_type = 'delete'\"\n",
    "        ).whenMatchedUpdate(\n",
    "            condition = \"origen._change_type = 'update_postimage'\",\n",
    "            set = {\n",
    "                \"nombre\": \"origen.nombre\",\n",
    "                \"ciudad\": \"origen.ciudad\",\n",
    "                \"saldo\": \"origen.saldo\",\n",
    "                \"estado\": \"origen.estado\"\n",
    "            }\n",
    "        ).whenNotMatchedInsert(\n",
    "            condition = \"origen._change_type = 'insert'\",\n",
    "            values = {\n",
    "                \"cliente_id\": \"origen.cliente_id\",\n",
    "                \"nombre\": \"origen.nombre\",\n",
    "                \"ciudad\": \"origen.ciudad\",\n",
    "                \"saldo\": \"origen.saldo\",\n",
    "                \"estado\": \"origen.estado\"\n",
    "            }\n",
    "        ).execute()\n",
    "        \n",
    "        print(f\"✓ {df_changes_to_apply.count()} cambios aplicados al DW\")\n",
    "    else:\n",
    "        print(\"No hay cambios para aplicar\")\n",
    "    \n",
    "    return version_final\n",
    "\n",
    "# Procesar cambios hasta la versión 3\n",
    "ultima_version_disponible = delta_table_cdf.history().select(max(\"version\")).collect()[0][0]\n",
    "\n",
    "ultima_version_procesada = procesar_cambios_incrementales(\n",
    "    ultima_version_procesada, \n",
    "    ultima_version_disponible\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Última versión procesada: {ultima_version_procesada}\")\n",
    "\n",
    "# Verificar que el DW está actualizado\n",
    "print(\"\\n=== DATOS EN DW (DESPUÉS DE PROCESAMIENTO INCREMENTAL) ===\")\n",
    "df_dw_final = spark.read.format(\"delta\").load(ruta_delta_dw)\n",
    "df_dw_final.orderBy(\"cliente_id\").show()\n",
    "\n",
    "# Comparar con tabla origen\n",
    "print(\"\\n=== DATOS EN TABLA ORIGEN (VERSIÓN ACTUAL) ===\")\n",
    "df_origen_actual = spark.read.format(\"delta\").load(ruta_delta_cdf)\n",
    "df_origen_actual.orderBy(\"cliente_id\").show()\n",
    "\n",
    "# Simular más cambios y nuevo procesamiento incremental\n",
    "print(\"\\n\\n=== SIMULANDO NUEVOS CAMBIOS ===\")\n",
    "\n",
    "# Nuevo UPDATE (Versión 4)\n",
    "delta_table_cdf.update(\n",
    "    condition = \"ciudad = 'Cusco'\",\n",
    "    set = {\"estado\": \"'PREMIUM'\"}\n",
    ")\n",
    "\n",
    "# Nuevos INSERTS (Versión 5)\n",
    "datos_adicionales = [\n",
    "    (8, \"Roberto\", \"Lima\", 12000.00, \"PREMIUM\"),\n",
    "    (9, \"Lucia\", \"Arequipa\", 8500.00, \"ACTIVO\"),\n",
    "]\n",
    "\n",
    "df_adicionales = spark.createDataFrame(\n",
    "    datos_adicionales,\n",
    "    [\"cliente_id\", \"nombre\", \"ciudad\", \"saldo\", \"estado\"]\n",
    ")\n",
    "\n",
    "df_adicionales.write.format(\"delta\").mode(\"append\").save(ruta_delta_cdf)\n",
    "\n",
    "print(\"✓ Nuevos cambios aplicados (versiones 4 y 5)\")\n",
    "\n",
    "# Procesar cambios incrementales nuevamente\n",
    "ultima_version_disponible = delta_table_cdf.history().select(max(\"version\")).collect()[0][0]\n",
    "\n",
    "ultima_version_procesada = procesar_cambios_incrementales(\n",
    "    ultima_version_procesada,\n",
    "    ultima_version_disponible\n",
    ")\n",
    "\n",
    "# Verificar DW actualizado\n",
    "print(\"\\n=== DW DESPUÉS DE SEGUNDO PROCESAMIENTO INCREMENTAL ===\")\n",
    "df_dw_final_2 = spark.read.format(\"delta\").load(ruta_delta_dw)\n",
    "print(f\"Total registros en DW: {df_dw_final_2.count()}\")\n",
    "df_dw_final_2.orderBy(\"cliente_id\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark Zero to Hero",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}